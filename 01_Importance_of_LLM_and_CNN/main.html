<!DOCTYPE html><html lang="ja"><head>
    <meta charset="utf-8">
    <title></title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="_css/main.css">
  </head>
  <body>
    <section class="level1" aria-labelledby="h1_1">
      <div class="coverpage"><h1 id="h1_0" class="chapter">1章<br>文章解析と画像解析の重要性</h1></div>
      <div class="subtitle"></div>
      <section class="level2" aria-labelledby="h2_1">
        <div class="secheader"><h2 id="h2_0" class="section"><br>社会へ浸透する生成AI</h2><br></div>
        <p>
          コンピューターの性能向上に伴ってAIの高性能化が進み、昨今は人間が入力した自然文（日常的に意思疎通に使う話し言葉や書き言葉）のリクエストに対して自然文や画像を生成して回答する生成AIに対する注目が高まっています。生成AIはすでにインターネット検索エンジンに統合され、検索語句に対して自然文で検索結果を返答するなど活用場面が拡大し、我々の日常にも浸透しつつあります。
          2022年11月に公開されたChatGPTは、分野によっては人間の作文と遜色ない文章を生成する事ができ、昨今の生成AIブームの火付け役となりました。アクセス解析会社のブログ<span class="notetext">
出典：<a href="https://www.similarweb.com/blog/insights/ai-news/chatgpt-birthday/">https://www.similarweb.com/blog/insights/ai-news/chatgpt-birthday/</a>
</span>によれば、ChatGPTの公開以降、ChatGPTおよび類似サービスへのアクセス数は急激に増加し、史上最速<span class="notetext">
2023年7月に、Twitter（現X）の対抗サービスとして登場したThreadsがサービス提供開始からわずか5日で利用者数1億人を突破してChatGPTの最速記録が更新されました。
</span>で利用者数が1億人に到達した事も話題になりました。
          </p><div class="figure">
          
          <img src="img/01_chatgpt_birthday_10.png" height="200">
        <!-- similarweb
        https://www.similarweb.com/blog/insights/ai-news/chatgpt-birthday/
         --><!-- ChatGPT is fastest app to hit 100m users in history
        https://businessday.ng/technology/article/chatgpt-is-fastest-app-to-hit-100m-users-in-history/
         -->
        <p>図X-X ChatGPTと類似サイトへのアクセス数推移</p></div>
        <p>サービス公開から順調に利用者を伸ばしたChatGPTですが、2023年6月には月単位利用者数が初めて減少に転じました。公開から数か月間の利用で、生成される文章に誤情報が含まれるといった問題が顕在化した事でChatGPT離れが起きたという主張もありますが、学校の宿題を解くためにChatGPTを利用していた米国の小学生が夏休みに入った事による影響という説が有力とみられています。その後に利用者数は回復し、現在でも世界でも有数のアクセス数<span class="notetext">similarweb社のランキングによれば、2023年11月におけるChatGPTへの月間アクセス数は約17億であり、世界19位との事でした。
<a href="https://www.similarweb.com/ja/top-websites/">https://www.similarweb.com/ja/top-websites/</a>
</span>を維持しており、多くのユーザが利用している事が分かります。</p>
        <p>自治体や民間企業においても、生成AIサービスの利活用検討が進んでいます<span class="notetext">パナソニックコネクト社では、日本の大企業としては早い段階で、ChatGPTをベースとしたAIアシスタントサービス「ConnectAI」を社内イントラに実装し、社員が社内情報についてAIに質問を行える環境を整備し、話題になりました。一方で、社外への情報流出を懸念してChatGPTの業務利用に慎重な企業も存在します。
</span>。宮崎県都城市では、市の独自情報を生成AIと連携させて回答を生成する「自治体独自AI」の実証実験を全国に先駆けて実施しています<span class="notetext">都城市では、LGWAN（行政機関専用のネットワーク）環境上に、市の様々なマニュアル等を登録してChatGPTと連携するシステム「自治体AI zevo（ゼヴォ）」を構築し、市の職員向けに統合FAQシステムを提供しています。（<a href="https://prtimes.jp/main/html/rd/p/000000085.000056138.html%EF%BC%89">https://prtimes.jp/main/html/rd/p/000000085.000056138.html）</a>
</span>。埼玉県戸田市では、ChatGPTの調査研究チームを設置して業務効率化の検討を進め、2023年10月に「自治体におけるChatGPT等の生成AI活用ガイド」<span class="notetext">ChatGPTの業務利用の事例や工夫が平易に紹介されているため、業務利用を検討される際の参考資料としてお薦めです。（<a href="https://www.city.toda.saitama.jp/uploaded/attachment/62855.pdf%EF%BC%89">https://www.city.toda.saitama.jp/uploaded/attachment/62855.pdf）</a>
</span>を成果物として公開しています。</p>
        <section class="level3" aria-labelledby="画像生成aiの商用利用も進む">
          <h3 id="h3_0">画像生成AIの商用利用も進む</h3>
          <p>
            画像分野においても生成AIの利用が進んでいます。2021年8月にはハリウッドスターのディープフェイク（deepfake）を用いて作成されたCM動画
            <span class="notetext">出典：<a href="https://www.youtube.com/watch?v=XSUQwwOm3G4">https://www.youtube.com/watch?v=XSUQwwOm3G4</a>
</span>が公開されました。
            この動画には、ハリウッド俳優のBruce Willis氏
            <span class="notetext">映画"Die Hard"（1988年）、"The Fifth Element"（1997年）、等に出演。上記のCM動画には、これらの出演映画を学習したdeepfakeが使用されており、当時のWillis氏の外見が再現されています。出典：<a href="https://www.bbc.com/news/technology-63106024">https://www.bbc.com/news/technology-63106024</a>
</span>の肖像を学習して生み出したdeepfakeが使用されており、ロシアの通信企業であるMegafon社の広告に使用されました。同氏は失語症を患い2022年に俳優業の引退を公表していますが、CM公開当時は同氏が自身のdeepfakeを作成する権利を（将来分に渡って）売却したとのニュースが流れ、同氏の体調不良の噂と相まって「本人は引退してしまうけれども、deepfakeを使えば新作映画や広告などに出演し続けられるじゃないか！」と話題になりました。（後日、売却の事実はないと同氏の代理人により否定されていますが、このCMのように、単発契約でdeepfakeを作成する可能性あるとの事です。）
          </p>
          <div class="figure">
            
            <img src="img/01_bruce_willis_ad.png" height="105">
            <img src="img/01_Bruce_creation.jpg" height="105">
          <!-- CNN
          https://www.bbc.com/news/technology-63106024
          INSTAGRAM
          https://www.instagram.com/p/Cc0EhIhsqB8/
          REUTERS
          https://www.reuters.com/video/watch/idOVEVO57JB
           -->
          <p>図X-X（左）：実際のCM動画からの抜粋（演じているのはロシアの俳優）<br>図X-X（右）：CM動画中でWillis氏を演じる俳優（左）と、俳優の顔上に重畳されたWillis氏のdeepfake画像（右）</p></div>
          <p>このCM動画はWillis氏側の許諾を得て製作されましたが、このように本人の精巧なコピーを生成して自由に演技させる他、実際の俳優の演技をAIで修正したり、実写と見分けがつかないモブ役（エキストラ）を生成して演技させる事が技術的に可能なため、2023年にはAI使用の制限を求めてハリウッドで大規模なストライキが発生しました。これほどまでに精巧な動画を生成できるようになった昨今では、生成AI技術を悪用して虚偽の情報を流布する事件も社会問題として顕在化しています。</p>
        </section>
        <section class="level3" aria-labelledby="画像から文章を生成するaiも社会実装へ">
          <h3 id="h3_1">画像から文章を生成するAIも社会実装へ</h3><!-- 2023年は奇しくもGoogle、Microsoft両者共にロービジョン<span class="notetext">英語では"Blind or Low Vision (BLV)"とも表現され、直訳すると「全盲ないし低視覚」となり、年齢による視覚機能の低下も含まれます。ここでは「見えにくさのため、生活に何らかの支障を来している人」の意を込め、視覚障がいではなくロービジョンと表現しました。
          </span> -->
          <p>画像から文章を生成するロービジョン<span class="notetext">英語では"Blind or Low Vision (BLV)"とも表現され、直訳すると「全盲ないし低視覚」となります。年齢による視覚機能の低下も含まれるため、ここでは「見えにくさのため、生活に何らかの支障を来している人」の意を込め、視覚障がい者ではなくロービジョンと表現しました。</span>向けのサービスに生成AIを導入する試みが行われています。例えば、スマートフォンのカメラで撮影した周囲の環境に関する情報を音声で伝え、ロービジョンがより自立して生活できるように支援するGoogle製のアプリケーション"Lookout"が挙げられます<span class="notetext">同様なサービスに、デンマークの"Be My Eyes"社の"Be My AI"が挙げられます。こちらは生成AIとしてChatGPT 4使用されています。日本でもBeta版の利用が可能であり、YouTubeなどで使用場面を視聴する事が出来ます。</span>。同アプリケーションは2019年に公開されましたが、2023年9月には、読み取った写真の説明文を生成して読み上げる"Image Q&amp;A"機能が追加されています。写真に写った物体や文字を認識して、その物体に関する情報や訳出を提供するアプリケーション（"Google Lens"など）は従来からありましたが、対象物だけではなく周囲の様子を含めて写実的に提供できる点が特徴です。生成された説明文に対して繰り返し質問を行う事も可能なため、写真の細部まで把握できるようになっています。</p>
          <div class="figure">
            
            <img src="img/01_Google_lookout.jpg" height="200">
          <!-- 
          https://ai.google/static/documents/responsible-development-of-lookout.pdf
           -->
          <p>図X-X Google Lookoutの画面イメージ</p></div><!--
          - **Lookout**：2019年に公開されたスマートフォンのカメラで撮影した周囲の環境に関する情報を音声で伝え、ロービジョンがより自立して生活できるように支援するGoogle製のアプリケーション。2023年9月に、読み取った写真の説明文を生成して読み上げる"Image Q&A"機能を追加。
          - **Be My AI**：ロービジョンが支援を必要とした場合に、ボランティアとマッチングしてビデオ通信等を用いた支援を仲介するサービス"Be My Eyes"にOpenAIのChatGPT 4を組込んだもので、2023年3月に公開。Microsoft社は以前から障碍者ヘルプデスク（Disability Answer Desk）に"Be My Eyes"を組み込んでいましたが、
          <span class="notetext">出典：https://www.bemyeyes.com/blog/introducing-microsofts-ai-powered-disability-answer-desk-on-be-my-eyes</span>
           -->
        </section>
        <section class="level3" aria-labelledby="幅広いデータを扱える生成aiの中身">
          <h3 id="h3_2">幅広いデータを扱える生成AIの中身</h3>
          <p>上述した文章や映像に留まらず、数秒の音声データから人の声を忠実に再現するAIや、曲の雰囲気を入力すれば作詞作曲を行うAIが公開されるなど、昨今の生成AIが扱うデータは多岐に渡ります。これらの生成AIは、文章や画像を無から生成しているわけではありません。<em>生成AIは何かしらの方法で文章や画像といったデータを解析して意味付けを行い、データの特徴を事前に学習した上で、利用者が求める情報（文章や画像、音声など）を生成している</em>のです。</p>
          <p>昨今の生成AIは言語モデル<span class="notetext">言語モデルとは、与えられた文章を解釈したり、その続きに相応しい文章や単語を予測する仕組みであり、機械学習モデルの一つです。モデルとは、このように与えられたデータに対して出力結果を返す仕組みであり、その中でも機械的に仕組みを学習したものが機械学習モデルと呼ばれます。この機械学習モデルについては第2章で説明します。
</span>を基本とし、画像モデルを組み合わせる事で、文章から画像を生成したり、音声モデルを組み合わせる事で音声を生成したりします。このように、別々の性質の情報（モダリティ）を同時に扱うAIをマルチモーダルAIと表現します。本書では昨今の生成AIの基本となる言語モデル、そして画像などの別のモダリティを、どのように組み合わせているのかという事に注目します。</p>
          <p>生成AIがどのようにデータを学習し、生成を行うのかを理解する事で、より効果的な活用が可能となるでしょう。仕組みを理解する事で、生成AIを利用する際に生じる課題や制約を正確に把握する事ができるようになります。本書を通じて、人間と生成AIとの「最適な付き合い方」を知るヒントを得ていきましょう。</p>
          <div class="column">
            <h4 id="h4_0">ディープフェイク（deepfake）</h4>
            <p>ある人物の顔や音声などを、著名人等の別人のものに置き換えたコンテンツの事で、既に聞き馴染みがある方も多いかもしれません。2017年にインターネット上の掲示板に"deepfakes"というユーザが機械学習アルゴリズムで作成したポルノを投稿し、以降広く使用されるようになりました。"Oxford English Dictionary"という英語辞書がありますが、deepfakeが単語として同辞書に掲載されたのは2023年3月。生成AIの進化によって単語が創出された事を考えると感慨深いですね。</p>
            <p>
              因みに、虚偽情報を指す"fake news"という単語は19世紀後半から存在していたようですが、D. Trump元大統領がメディアに対して発言した事を端に、2016年以降に広く使用されるようになったそうです
              <span class="notetext">偽物を表す"fake"という単語が広く使用されるようになったのも18世紀後半との事で、それ以前は虚偽情報を"false news"などと表記していたそうです。（https://www.merriam-webster.com/wordplay/the-real-story-of-fake-news）
</span>。
            </p>
          </div><!-- 因みに虚偽情報を指す"fake news"という単語はTrump元大統領がメディアに対して発言した事を端に、2016年以降に広く使用されるようになったようです。 --><!-- このように高品質の文章や画像や動画が生成できてしまうが故に、悪意を持って使えば偽りの情報（いわゆるfake news）を生成して流布する事が出来てしまいます。 --><!-- "Armageddon"（1998年）、 --><!-- fake newsが単語として登場したのは2016年 --><!--
          これらのモデルは表面的な形式（例えば言語モデルであれば単語の出現頻度やパターン）を学習しているだけで、真に物事を理解している訳ではないという説が古くからある一方で、時空間を理解している可能性があるとの研究結果
          <span class="footnote">2023年10月にMIT（マサチューセッツ工科大学）が公開した論文*"Language Models Represent Space and Time"*によれば、大量のデータを学習した事によって我々が生活する空間や時間といった概念を構造的な知識（"world model"）として有している可能性が述べられています。<br>（https://doi.org/10.48550/arXiv.2310.02207）</span>
          や、言語モデルと人間の脳の活動パターンが類似するという研究結果
          <span class="footnote">2023年10月にコペンハーゲン大学らの研究チームが公開した論文*"Structural Similarities Between Language Models and Neural Response Measurements"*によれば、特定の単語やフレーズに応答する際の、AIモデルの活性化状態の可視化イメージと、人間の脳の活性化状態のfMRI（機能的磁気共鳴画像法：脳の血流から活動状況を調べる手法）イメージを比較したところ、モデルの規模が大きくなるに連れて類似するという結果が得られたそうです。著者らは、言語モデルは飽くまでも表面的な形式に基づいており、本質的な意味理解が不足しているという論説（いわゆる"Newman's objection"）を否定しています。
          <br>（https://doi.org/10.48550/arXiv.2306.01930）</span>
          が公開されています。
          他人の頭を覗く事が出来ないように、生成AIの中身（思考過程）を理解する事は非常に困難です。一方で、中身を十分に理解していないと使えないか、というとそうではないのです。実際にビジネス運用にも耐え得るサービスが登場している事からも分かる通り、これらの生成AIの得手を上手く活用する事で、従来の作業を効率化したり、新たな価値を生む事も可能なのです。
          -->
          <hr class="pagebreak">
        </section>
      </section>
      <section class="level2" aria-labelledby="生成aiの種類と文章生成aiと画像生成aiの関係">
        <div class="secheader"><h2 id="h2_1">生成AIの種類と、<br>文章生成AIと画像生成AIの関係</h2><br></div>
        <p>活用範囲が拡大し、時には社会現象をも引き起こす生成AIですが、文章や画像を生成するだけではなく、動画、音声（歌）、三次元の点群データ、などを生成するAIも登場しています。主な生成AIの例としては、次のようなものが挙げられます。</p>
        <div class="figure">
          
          <img src="img/01_generativeAI_table.jpg" width="400">
          </div>
        
        <p>ChatGPTやBardなどのWebサービスでは、ブラウザ上に表示される入力画面にメッセージ（プロンプト）を入力して送信ボタンをクリックする事で結果が出力されます。この一連の処理は、Web画面を通じてクラウド上にある生成AIを呼び出す事によって実現されています。文章生成AIを利用するには大容量のメモリが必要となり、高負荷な計算が必要になる事から、個人のPC上ではなくクラウド上のサービスが使用される場合が多いです。中には比較的処理が軽量で個人で利用可能なライセンス下で配布されているモデルもありますが、これらのモデルについては第5章で紹介します。</p>
        <div class="figure">
          
          <img src="img/01_generative_process.jpg" width="400">
        
        <p>図X-X Google Bardの入出力イメージ</p></div>
        <p>画像を生成するStable Diffusionには、様々な画像で学習された複数のモデルが公開<span class="notetext">モデルの配布サイトとしては、"Hugging Face"（<a href="https://huggingface.co/%EF%BC%89%E3%82%84%22Civitai%22%EF%BC%88https://civitai.com/%EF%BC%89%E3%81%8C%E6%9C%89%E5%90%8D%E3%81%A7%E3%81%99%E3%80%82">https://huggingface.co/）や"Civitai"（https://civitai.com/）が有名です。</a>
</span>されており、GPUが搭載されているパソコン上で動作させる事も可能です。今回は次のようなプロンプト<span class="notetext">日本語のプロンプトを使用する事も可能ですが、英語で指示した方が上手い具合の画像が生成されたため、今回は英語を使用しました。
</span>を用いて、異なる3種類のモデルに画像を生成させてみました。</p>
        <ul>
          <li>"Bright spring day with pink cherry blossoms. A girl with large cherry blossom as glasses, petals like sunglasses around the eyes. Pink petals dancing in the wind for a dynamic touch."<br>（意訳：桜が咲く明るい春の日に、大きな桜の花びらのようなサングラスを掛けた少女を、花びらが風に舞うダイナミックなタッチで描いてください）</li>
        </ul>
        <p>同一のプロンプトを与えたのにも関わらず、モデルを変更する事によって（a）油彩風の画像を生成したり、（b）液体が弾けるような透明感のある写実的な画像を生成したり、（a）アニメ風の画像を生成したりする事ができます<span class="notetext">この例では、"SD XLv1.0 VAE fix"というモデルを基本とて、画風を変更するために(a) Sacred Oil Painting Style、(b) Liquid Flow Style [LoRA 1.5+SDXL]、(c) Anime SDXL (OPTION ONE)、という追加学習モデル（LoRA）をそれぞれ使用しています。
</span>。</p>
        <div class="figure">
          
          <img src="img/01_SD_images.jpg" width="400">
        
        <p>図X-X Stable Diffusionで生成した画像</p></div>
        <p>文章生成AIと画像生成AIとは、お互いに関係が無さそうにも思えますが、よく考えると画像を生成させる際にプロンプトとして文章を入力しています。この事から、少なくとも画像を生成する際に文章を解析して理解し、それに対応する画像を（事前に学習した記憶を頼りに<span class="notetext">生成AIと言っても、学習していないモノを完全に無から生み出す事は困難です。事前に「この画像には桜が描かれている」「この画像にはサングラスが映っている」「この色はピンク」といった具合で大量の学習が行われているため、先ほどのような絵を生み出す事ができる、と考えられます。
</span>）生成する能力を持っていると考えられます。</p>
        <p>
          このように昨今の生成AIにおいては自然言語がインターフェースとして用いられており、この自然言語を扱う<em>大規模言語モデル（LLM; Large Language Model）</em>が重要な役割を果たしています。大規模言語モデルは文章を理解し、次の単語を繰り返し予測する事によって文章を生成したり、翻訳、要約、返答文生成など様々なタスクを実行する事が可能です。この文章を理解するという能力を活用し、人間の意図や指示を理解して文章や画像、音楽などのデータを生成したり、逆にデータから自然文を生成して人間に意図を説明する事ができるのです。
          例えば、先ほどの（a）油彩風の画像をAIに説明させると、次のような文章が生成されます<span class="notetext">この説明文の生成にはChatGPT 4を用いました。因みにGoogle Bardでは人物が映っている画像は扱えない旨の返答があり、同社がAI倫理を慎重に運用している姿勢が伺えます。</span>。
        </p>
        <ul>
          <li>この絵は、春の桜の花の下で楽しげに過ごす女性を描いたものです。女性はピンクのサングラスをかけていて、これが大きな桜の花びらのように見えます。彼女の服装は和服を思わせるピンク色のガーメントで、風になびく桜の花びらと調和しています。彼女の表情は明るく生き生きとしており、春の訪れとその美しさを満喫している様子が伝わってきます。</li>
        </ul>
        <p>大規模言語モデルはTransformerという技術の登場によって飛躍的に性能が向上し、昨今の生成AIにおいてデファクトスタンダードとなっています。この能力を如何に獲得したかという点をTransformerという技術に着目して簡単に説明します。</p><!-- 
        また、視覚的に操作しやすいようにインターフェース<span class="notetext">例えば、AGPL-3.0ライセンス下で"Stable Diffusion Web UI"が公開されています。(https://github.com/AUTOMATIC1111/stable-diffusion-webui)
        </span>が公開されています。 
        <div class="figure">
        ![](img/01_stable_diffusion.png){height=250}
        <p>
        Stable Diffusion web UIの操作画面
        </p>
        </div>
        --><!-- 
        - 文章生成AIの例<span class="notetext">GPT-4やBardでは画像を認識する機能が導入されているため、単なる文章生成AIという括りではないかもしれません。OpenAIは"a large multimodal model"であるとしています。(https://openai.com/research/gpt-4)
        </span>
          - GPT（OpenAI社）	
            - 主なサービス：ChatGPT
          - PaLM2（Google社）
            - 主なサービス：Bard
          - Llama（Meta社）
            - 無償で商用利用も可能
        - 画像生成AIの例
          - DALL-E（OpenAI社）
            - 主なサービス：ChatGPT Plus
          - Stable Diffusion（オープンソース）
          - Midjourney（Midjourney, Inc.）
        - 動画生成AIの例
          - AnimateDiff<span class="notetext">AnimateDiffは、Stable Diffusionをベースとして利用しつつ、数百万本の動画を学習させた"Motion module"を適用する事で、文章プロンプトから動画を生成します。
        </span>（オープンソース）
        - 音声生成AIの例<span class="notetext">音声合成（Text to Speech）とも。従来から自動読み上げソフトといった文章を音声に変換する技術は存在しますが、今までは声の波形を操作して音声を合成する方式が一般的でした。VALL-Eは、言語モデルと、音色、声の高さ、イントネーションなどの要素を組み合わせて学習した"a neural codec language model"を用いており、テキストの意味を正確に理解して自然な音声を生成する事ができます。（https://arxiv.org/abs/2301.02111）
        </span>
          - VALL-E（Microsoft社）
        
        | 生成対象 | モデル | 主な開発元 | 主なサービス |
        | :------: | :----: | :--------: | :----------: |
        |  文章   |  GPT系 |  OpenAI |   ChatGPT |
        |  文章    | PaLM系 | Alphabet |   Bard  |
        |  文章    | Llama系 | Meta |  無償で商用利用も可能    |
         -->
        <section class="level3" aria-labelledby="生成aiの基本はtransformer">
          <h3 id="h3_3">生成AIの基本はTransformer</h3>
          <p>Transformerは2017年に登場した大規模言語モデルのアーキテクチャ（基本構造）であり、昨今の生成AIにとって重要な技術です。登場以来、学術界に留まらず産業界においても幅広い自然言語処理に取り組むためのデファクトスタンダードとなっています。このTransformerは、2015年に提唱されたAttentionと呼ばれる機構を用いており、同じデータ系内にある隔たったデータ要素間の微妙な相互影響や相互依存関係を見つけ出す能力に長けています。入力されたデータの異なる部分に、異なる重要度を自動的に割り当てる能力なのですが、端的に言えば、モデル自体がデータ（文章や画像など）の特徴を自ら抽出して、自ら学ぶようなイメージです。</p><!-- 
          このAttention機構の計算処理を並列処理できるようにする事で、処理速度を
           -->
          <p>例えば、皆さんが次の文章を読む場面を想像してみてください。</p>
          <ul>
            <li>Bardは2023年10月に更新され、日本語で画像のアップロードに対応した</li>
          </ul>
          <p>本書をここまで読んで下さったのであれば、上記の文章を読めば「Bardは文章生成するAIだったな、画像も読み込む事ができるのだな」といった具合に、今まで読了した記憶を頼りに文章を理解する事ができるかと思います。もう一つの具体例として、次の文章を考えます。</p>
          <ul>
            <li>The Bard wrote many iconic poems.<br>（意訳：Bardは多くの象徴的な詩を書いた）</li>
          </ul>
          <p>本書の文脈を鑑みると、文章生成AIのBardが多くの象徴的な詩を生成したのだろう、と解釈される事になるかと思います。一方で"bard"という英単語自体は詩人を表し、"The Bard"と表記すると、代表的な詩人であるW. Shakespeareを指す事がありますので、この文章単独だけ切り取るとShakespeareに関する言及とも解釈できます。また、英語の場合だと定冠詞（"the"）が付く名詞の多くは文脈から特定可能であるため、この文章が別の詩人について述べている書籍において登場した際には、"The Bard"はその詩人を指すと解釈する事になります。</p><!-- <span class="notetext">同氏の出生地である英国の地名を付加して"Bard of Avon"（Avonの詩人）とも称されます。この文中では「同氏」（＝Shakespear）という表現を用いましたが、問題なく解釈できるかと思います。一方で、何故分かるのかと問われると理論的に答える事は意外と難しいのではないでしょうか。Transformerでは、入力された文章を構成する要素間の関係性を計算する事によって能力を獲得しているのですが、人間の脳も実は同様な計算を行って理解しているのか、気になるところです。</span> -->
          <p>
            上記の一連の文章中でも、「<em>この</em>文章」「<em>その</em>詩人」という指示語が用いられていますが、それぞれが何を示すのか解釈に迷う事はなかったかと思います。皆さんの頭の中では、読み取った文章に基づいて「この文章」が何を示すのか、「その詩人」が誰を指しているのかについて、文脈から類推して補って理解を進めているのです。
            このように、文章という連続した情報の中から、離れた位置にある特徴的な要素の大局的な関係を解釈する機構がAttentionであり、この能力に長けたモデルがTransformerです。
          </p>
          <div class="figure">
            
            <img src="img/01_feature_extraction.jpg" width="400">
          
          <p>図X-X 離れた位置にある情報の関係性を把握するイメージ</p></div>
          <p>この大局的な関係性を捉えるという能力を応用して、画像や音声といった別種のデータ形態に対してもTransformerが応用されています。2023年6月に公開されたサーベイ論文<span class="notetext">サーベイ論文とは、特定のテーマに関する既存研究を体系的にまとめた論文の事です。ここで紹介する論文は、Transformerを利用したモデルに対する包括的な論文で、2017年から2022年までに提案された650以上のモデルについて述べられています。（<a href="https://doi.org/10.48550/arXiv.2306.07303%EF%BC%89">https://doi.org/10.48550/arXiv.2306.07303）</a>
</span>によれば、近年提案されたTransformerモデルの40％が自然言語処理、31％が画像処理、15％が文章や画像などを組み合わて処理するマルチモーダル、11％が音声、残り4％が信号処理に関するものであったとの事です。いずれのモデルにおいても、文章や画像などのデータから特徴を抽出してTransformerに学習させる事によって、データを理解して生成するという能力を獲得しています。</p>
          <p>データから特徴を抽出する手法として、例えば文章の場合はトークナイズ（tokenize）と呼ばれる手法があり、画像の場合は畳み込みと呼ばれる手法があります。</p><!-- 
          Transformerの勃興以前は、人間が何らかの形で特徴をモデルに教える必要があると考えられていました。 -->
          <ul>
            <li>トークナイズとは、文章を構成する単位であるトークンに文章を分割する事で、国語の授業で扱った文節に分けるイメージが近いかもしれません。本書の第3章で扱います。</li>
            <li>畳み込みとは、画像の輪郭や色味といった特徴を抽出する事で、本書の第6章で扱います。また、画像を文章のトークンのように分割して学習するViT（Vision Transformer）についても同章にて紹介します。</li>
          </ul>
          <p>例えば、「このコーヒーは我ながら上出来だ」という文章を、人間が読んだ時と同様に幾つかの要素に分割します<span class="notetext">コンピューター内で文章といった連続した情報を直接扱う事は難しいので、与えられたデータを区切ったり抽象化して不連続なデータに変換する事を離散化と言います。画像においても同様に、画素の明るさ（輝度値）といったアナログの連続情報を、所定の数段階に分けることによって離散化を行います（詳細は第5章）。
</span>。このトークナイズ作業の後に得られた「この/コーヒー/は/我/ながら/上出来/だ」という構成要素を学習に用います。要素ごとに分解すると、「コーヒー」「上出来」といった、当該文章中の重要と思われる特徴語句をモデルに学習させる事ができます。このように大量の文章を学習させる事によって言語モデルが作り上げられます（詳細は第3章）。更に、この文章に対して「嬉しい」というラベルを付けて学習させる事により、この文章には「嬉しい」という感情が含まれている事を学習する事ができます。文章に感情などの属性を付与して学習させる事により、言語モデルは、文章の意味だけでなく、感情や意図なども理解する事ができるようになります。</p>
          <p>同様に、コーヒーカップの写真から、畳み込みという処理を用いる事によって特徴を抽出する事ができます（詳細は第6章）。抽出された特徴を人間が直接解釈する事は難しいですが、カップの輪郭や色味といった特徴が抽出され、これらの特徴を学習に用います。更に、この画像に対して「コーヒーカップ」というラベルを付けて学習させる事により、この特徴は「コーヒーカップ」という物体を表すという事を学習する事が出来ます。</p><!-- <hr class="pagebreak"> -->
          <div class="figure">
            
            <img src="img/01_features.jpg" width="400">
          
          <p>図X-X 文章や画像を離散化して入力するイメージ</p></div>
          <p>このように学習した言語モデルと画像モデルは、それぞれコーヒーについての特徴を理解していますので、「このコーヒーは我ながら上出来だ」という文章と、コーヒーカップの写真を組み合わせて両者の特徴を学習する事によって、「この画像は上出来なコーヒーを表しているんだな」というように、文章と画像の関係性を学習していきます。このように学習を行う事で、文章を見ただけで対応する画像を特定したり、画像を見て対応する文章を特定する事ができます。文章や画像を生成するには、データの特徴をモデルが理解しておく必要があり、どのようにデータを解析して特徴を抽出するかという点に工夫があります。</p><!-- 文章や画像の特徴をどのように抽出するかという点は、第3章と第5章でそれぞれ説明します。上手く抽出した特徴を、データ間の関係性を把握する事に長けたTransformerに入力して学習させてやればよいのです。 -->
        </section>
        <section class="level3" aria-labelledby="生成aiはデータを数字の羅列で捉えて学習している">
          <h3 id="h3_4">生成AIはデータを数字の羅列で捉えて学習している</h3>
          <p>あらゆる機械学習モデルは、文章や画像といったデータから特徴を抽出して学習を行います。生成AIモデルの中身は複雑で、十分に理解できていない部分も多分にありますが、アルゴリズムの組み合わせによって機能する人工知能です。モデルの中では、文章や画像は内部的には数字で表現されていて、何らかの規則（ここが複雑なのですが）によってこれらの潜在表現<span class="notetext">潜在表現 (Latent Representation)とは、入力された文章や画像などのデータから本質的な特徴を抽出した内容であり、数字の集合であるテンソルで表現されます。内部表現 (Internal Representation)とも言われますが、こちらはモデルがデータを処理する過程の中間的なデータ表現の意もあり、特にニューラルネットワーク内の処理過程を説明する文脈で使用する事が多いかもしれません（第2章参照）。
</span>を獲得し、所望のデータを生成するために必要な事前学習を行っています。</p>
          <p>
            文章や画像をどのように数字表現に変換するかについては別章で述べますが、ここでは3つの単語の潜在表現を例示します。各単語を200個の数字（200次元）に変換<span class="notetext">今回は日本語のWikipedia全文をword2vec（CBOW）モデルで学習し、各単語の潜在表現を計算しました。
</span>し、縦10個、横20個に並べて、数字の大小で色付けを行ったものです。この例では、（a）コーヒーと（b）お茶は色合いが近しい一方で、（c）上出来という単語は傾向が異なっています。前者2単語は共に飲料を表し、後者は形容詞ですから、両者の傾向が違うというのは我々の感覚とも一致しているかと思います。
            文章であっても同様に変換する事が可能であり、文章を数字表現として捉える事で、文章が持つ意味合いを表すことができるのです。
          </p>
          <p>このように、文章や画像などの<em>元データの潜在表現を獲得する機能をEncoder</em>と呼びます。また、この処理を逆方向に実行して<em>潜在表現から文章や画像といったデータを生成する機能をDecoder</em>と呼びます。</p>
          <div class="figure">
            
            <img src="img/01_internap_rep.jpg" width="400">
          
          <p>図X-X 学習により獲得した3単語の潜在表現</p></div>
          <p>生成AIは、これらの機能を組み合わせて文章や画像といったデータの潜在表現を獲得し、その潜在表現から別のデータを生成しています。図X-Xに、文章から画像を生成する生成AIの簡単な概要を図示しました。</p>
          <ul>
            <li>Text Encoder：入力された文章から潜在表現を獲得する部分で、「サングラスをかけた少女」という文章をトークンに分割した上で、事前に学習したモデルを使用して潜在表現を計算します。</li>
            <li>Image Genarator：文章の潜在表現から画像を生成する部分です。文章と画像とを紐づけることができるモデル（CLIPなど）を利用して文章の潜在表現から画像の潜在表現に変換を行い、画像生成モデル（VAE、GAN<span class="notetext" style="font-weight:normal;">GANは、一見ノイズにしか見えないような画像から（人間が見ると）意味のある画像を生成するモデルで、画像生成を担う部分はGenerator（生成器）と呼ばれ、Decoderに似た役割を果たしています。GANの例のように、生成を担うモデルにはDecoderが無いものもありますが、今回は簡便のためDecoderと表現しています。</span>、Diffusionモデル<span class="notetext" style="font-weight:normal;">拡散モデルとも表現されます。元々の画像にノイズを付加していく拡散過程（Forward Process）と、その逆プロセス（Reverse Process）を経ることで元の画像を生成するように学習したモデルです。この逆プロセスが画像生成機能を担いますが、Decoderよりも複雑な機構でDecoderとは表現されません。</span>など）を使用して画像を生成します。</li>
          </ul>
          <div class="figure">
            
            <img src="img/01_text2image.jpg" width="400">
          
          <p>図X-X 画像生成AIの処理イメージ</p></div>
          <p>図X-Xに示したText Encoderと、Image Generatorの組み合わせは多く提案されており、役割が重複する組合わせや、Decoderという表現を使わないモデルも含まれるため、飽くまでもイメージとなります。重要な点は、入力されたデータから潜在表現を経由して目的のモダリティのデータを生成するという点です。例えば、文章から音声を生成する音声生成AIは、図X-X中のImage Generatorを、音声データを生成するモデルに変えてやる事で実現できます。画像を入力として潜在表現に変換して、その潜在表現から文章を生成するようにモデルを組み合わせれば、先に紹介した"Google Lookout"のように画像を説明する生成AIを作成できます。</p>
          <p>このように、文章や画像といったモダリティから潜在表現を行き来するには、人間の赤子が言語を習得するのと同様、事前に多量の学習を行う必要があります。このように事前に学習を行ったモデルを<em>基盤モデル（Foundation model）</em><span class="notetext">スタンフォード大学に所属する研究者らで構成されるHAI（Stanford Human-Centered AI Institute）が2021年に提唱したコンセプトで、論文中では"...trained on broad data at scale and are adaptable to a wide range of downstream tasks..."と紹介されている。GPT以外の基盤モデルとして、BERT（Googleが発表した言語モデル）、DALL-E（OpenAI社の画像生成モデル）が例示されています。（<a href="https://doi.org/10.48550/arXiv.2108.07258%EF%BC%89">https://doi.org/10.48550/arXiv.2108.07258）</a>
</span>と呼びます。</p>
          <div class="column">
            <h4 id="h4_1">大規模言語モデルと人間の脳は同じ構造？</h4>
            <p>人間の脳が単語や文章、文脈といった言語を理解する仕組みも完全に解明されていませんが、大規模言語モデルの構造は、人間の脳に似ていると考えられています。2023年にコペンハーゲン大学の研究者らによって発表された論文<span class="notetext">出典：https://doi.org/10.48550/arXiv.2306.01930</span>によれば、人間の脳神経反応をfMRI<span class="notetext">fMRI（functional Magnetic Resonance Imaging）は脳内の血流や酸素濃度の変化を検出することで神経細胞の活発性を可視化する手法です。</span>を用いて分析したところ、特定の単語やフレーズの言語処理（リスニングやリーディング）を行った際の脳の活性化領域に幾何的な関係があり、言語モデルの規模が大きくなるにつれて、人間の脳の反応構造に類似するとの事です。言語モデルが更に強力になり、人間に近い表現を生成できるようになれば、人間の言語理解の研究や、言語障害の治療法開発など、様々な分野で大きな影響を与える可能性がありそうです。</p>
          </div>
        </section>
        <section class="level3" aria-labelledby="基盤モデルとは">
          <h3 id="h3_5">基盤モデルとは</h3>
          <p>
            基盤モデルのイメージをGPTとChatGPTを用いて説明します。皆さんがブラウザ上から操作することが可能なチャットアプリケーションであるChatGPTには、GPTと呼ばれる事前学習済みのTransformerが用いられています。GPTはGenerative Pretrained Transformerの略で、Transfomerで大量の文章<span class="notetext">GPT-3の場合、約45TB（テラバイト）のテキストデータが学習に使用されています（<a href="https://doi.org/10.48550/arXiv.2005.14165%EF%BC%89%E3%80%82%E3%82%88%E3%82%8A%E6%96%B0%E3%81%97%E3%81%84GPT-4%E3%81%AE%E5%AD%A6%E7%BF%92%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AF%E9%9D%9E%E5%85%AC%E9%96%8B%E3%81%A7%E3%81%99%E3%81%8C%E3%80%81%E3%82%88%E3%82%8A%E5%A4%9A%E3%81%8F%E3%81%AE%E3%83%87%E3%83%BC%E3%82%BF%E3%82%92%E5%AD%A6%E7%BF%92%E3%81%97%E3%81%A6%E3%81%84%E3%82%8B%E3%81%A8%E8%80%83%E3%81%88%E3%82%89%E3%82%8C%E3%81%A6%E3%81%84%E3%81%BE%E3%81%99%E3%80%82">https://doi.org/10.48550/arXiv.2005.14165）。より新しいGPT-4の学習データは非公開ですが、より多くのデータを学習していると考えられています。</a></span>を事前学習する事によって、文章中にある情報を潜在表現として把握する能力や、潜在表現から文章を生成する能力といった、様々な言語能力を獲得しました。他方で、このような大規模言語モデルは、事実と異なる情報、有害な情報、あるいはユーザーにとって役に立たない情報を出力する事があり、人間が期待する結果とは異なる場合があるという問題（アラインメント問題）<span class="notetext">OpenAI社が2022年に公開した論文では、モデルを大規模化してもユーザーの意図に必ずしも沿うわけではない（"... these models are not <em>aligned</em> with their users."）と述べられており、RLHFによるモデルのファインチューニング手法が提案されています。（第2章1節で扱います）</span>がありました。
            このGPTに対して、人間好みの回答を出力できるようにファインチューニングを行って、人間と会話するタスクに特化したモデルがChatGPTとなります。
            この例では、大規模学習を行ったGPTが基盤モデルとなり、ChatGPTは下流タスクに特化したモデルと説明する事が出来ます。
          </p>
          <p>画像や音声といった別のモダリティのデータについても同様に学習する事によって、各モダリティに特化した基盤モデルを構築する事ができ、幅広い下流タスクに応用する事が出来るのです。図X-X<span class="notetext">基盤モデルに関する論文"On the Opportunities and Risks of Foundation Models"（<a href="https://doi.org/10.48550/arXiv.2108.07258%EF%BC%89%E4%B8%AD%E3%81%AE%E5%9B%B3%E8%A1%A8%E3%82%92%E7%AD%86%E8%80%85%E3%81%8C%E5%8A%A0%E5%B7%A5%E3%81%97%E3%81%9F%E3%82%82%E3%81%AE%E3%81%A7%E3%81%99%E3%80%82">https://doi.org/10.48550/arXiv.2108.07258）中の図表を筆者が加工したものです。</a></span>では、基盤モデルが様々なモダリティのデータを共通の潜在表現空間に埋め込んでいる様を表しています。この共通の潜在表現空間内では、モダリティの垣根を越えて概念を関連付けることができます。例えば、「２」や「４」が描かれた画像を対比したり類比させることによって、違う概念なのか同じ概念なのかを識別するタスクを実行できます。また、画像と言語は別のモダリティですが、「４」と「Four」は同じもの、「９」と「六」は別のもの、といった具合に識別することができます。</p>
          <div class="figure">
            
            <img src="img/01_foundation_models.jpg" height="200">
          
          <p>図X-X 基盤モデルがモダリティを問わず種々のタスクを実行するイメージ<br></p></div>
          <p>
            昨今の画像生成AIには、CLIP<span class="notetext">Contrastive Language–Image Pre-trainingの略で、2021年にOpenAI社が発表した手法です。原論文では、インターネット上で収集した4億ペア（画像と、その画像を説明する文章）のデータセットに対して学習を行っています。（<a href="https://doi.org/10.48550/arXiv.2103.00020%EF%BC%89">https://doi.org/10.48550/arXiv.2103.00020）</a></span>という技術が用いられています。CLIPは言語と画像を関連付けて処理する手法で、ある画像と、その画像に対する説明文の言語表現とを紐づけて大量に学習する事で、画像表現と言語表現との間を行き来でき、こちらも基盤モデルの一つに数えられます。大規模言語モデルが持つ幅広い概念（潜在表現空間）と画像を関連付けることによって「馬に乗った宇宙飛行士」などの非現実的な概念をも理解して画像を生成する事が可能になりました。
            また、離散化した特徴量や、一見ノイズにしか見えないような画像から（人間が見ると）意味のある画像を生成するVAE、GAN、Diffusionモデルは、文章から画像を生成するAIにとっては重要な技術となっています（詳細は第8章）。
          </p>
          <hr class="pagebreak">
          <div class="figure">
            
            <img src="img/01_transformer_evo.jpg" height="250">
          
          <p>図X-X Transformerが生成AIの基本技術になるイメージ</p></div>
          <p>この節では、生成AIは機械学習モデルの一種であって、入力されたデータから特徴を把握する能力に長けた機械学習モデルであるTransformerが基礎になっており、文章や画像を解析して得られた特徴量を使って学習しているというイメージを持っていただければと思います。</p>
          <div class="column">
            <h4 id="h4_2">学習する事によって知識を得た大規模言語モデル？</h4>
            <p>GPTは大量の文章を学習する事によって文章を読解し生成する能力を獲得しています。ここで、皆さんが第二言語を学習する場面を想像してみてください。文法や単語をある程度覚えたら、大量の文章を読解して、文章の構成を把握する能力を鍛える事になるかと思います。その際、その言語で書かれている多くの文章を読む事になりますので、言語を習得すると同時に、文章に書かれている知識も自然と頭に入って来るのではないでしょうか<span class="notetext">筆者は以前TOEFL（北米留学生向けの英語能力試験）の問題集を解いていた時期があったのですが、近代米国史に関する文章が多く出題されるので、米国第3代大統領がThomas Jeffersonであって建国の父と称されている事、禁酒法を制定したのが憲法修正18条で撤廃したのが修正第21条である事など、断片的な記憶があります。これを知識と呼べるかと問われると怪しいですが、特に米国史を勉強したい訳ではなかったのですが、英文を読解している中で文中の知識が意識せず記憶されるというのは、少し不思議な気がします。
</span>。</p>
            <p>近年の研究によると、大規模言語モデルが学習を行う過程でも同様な現象が起こっている可能性が示唆されています。2023年10月にMIT（マサチューセッツ工科大学）が公開した論文<span class="notetext">出典：https://doi.org/10.48550/arXiv.2310.02207</span>によれば、大規模言語モデルが大量のデータを学習した事によって、我々が生活する空間や時間といった概念を、構造的な知識として獲得した可能性が述べられています。一方で「それらしい」文章を統計的に生成しているに過ぎず知識とは言えない、という視点に立つ研究者もいます。</p>
            <p>大規模言語モデルが獲得した知識が人間の知識と同質かどうかについては意見が分かれるところですが、単純な計算処理を大量に組合わせる事によって、研究者にも予想する事ができなかった能力を獲得してきたという点は興味深いですね。今後も研究が継続され、新たな能力が発見されるかもしれません。</p>
          </div><!--
          ## <br> 生成AIに対するよくある疑問
          <br>
          
          ChatGPTのサービス提供開始によって生成AIの知名度が上がった結果、生成AIをビジネスで活用したいという相談をよく耳にするようになりました。一方で、生成AIに対する理解度は人によってバラつきがあり、漠然とした疑問を持っておられる方もいらっしゃいます。本書を手に取った読者の中には、既に生成AIに対して具体的な疑問を持っている方もいらっしゃるかと思いますが、それは次のような疑問ではないでしょうか？
          本書を読み進める中で、これらの疑問に対する答えが浮かび上がってくるかと思います。関連するキーワードは<span class="hashtag">#ハッシュタグ</span>形式で記載しましたので、読み進める手がかりとしてください。
          
          
          #### ① 大半は英語で学習されているから、日本語だと精度が良くないのでは？
          <span class="hashtag">#離散化</span> 
          <span class="hashtag">#word2vec（skip-gram, CBOW）</span> 
          
          現在のところ、大規模言語モデルは言語に依存しない、というのが通説です。先述したように、言語モデルは言語を数字変換し、潜在表現を獲得しています。
          このように獲得した潜在表現は、人間の言語に依存しない、独自の表現を持つ事が分かってきています<span class="notetext">2023年10月の論文では、大規模言語モデルが単に文章を生成するための統計（確率）を計算しているだけでなく、物事がどのように位置づけられ、時間がどのように進行するかを「理解」している可能性が新たに示されています。大量の文章を学習していく中で、我々の住んでいる世界の構造化された知識をも獲得している事を示唆しています。（https://arxiv.org/pdf/2310.02207.pdf）
          </span>。よって、学習時や問いかけ時の言語には依存せずに、大規模言語モデルを利用する事が可能です。ただし、学習に用いた文献は英語圏のものが多い<span class="notetext">OpenAIが公開した論文では、GPT-3の学習に使用されたデータの93％が英語との事です。学習データについては非公開とされる場合が多いのですが、特定の言語に特化したモデルではない限りは、他のモデルでも同様な構成比である事が推察されます。（https://arxiv.org/pdf/2005.14165.pdf）
          </span>ため、英語圏の慣習に従うような回答が多めになると言われています。
          
          #### ② 古いデータで学習されており、最新情報にはついていけない
          <span class="hashtag">#RAG</span> 
          <span class="hashtag">#Embedding</span> 
          <span class="hashtag">#ベクトルDB</span>
          
          モデルの作成には大量の学習が必要になるため、リアルタイムでモデルを更新していく事は困難です。例えば、ChatGPTでは2021年9月までのデータが学習されています<span class="notetext">無償版ChatGPTには"gpt-3.5"系の言語モデルが使用されており、2021年09時点のデータで学習されています。2023年11月に発表された"gpt-4-turbo"系の言語モデルは2023年4月までのデータで学習されています。（https://platform.openai.com/docs/models/）
          </span>。一方で、有償版のChatGPT、MicrosoftのBing、GoogleのBardでは、インターネット検索も可能なため、学習時には知りえなかった知識についても、言語モデルを更新する事なく回答する事が可能になっています。モデルの更新をせずに最新情報を扱うという事は、例えると、全く土地勘がない地方新聞を読むイメージに近いかもしれません。その土地固有のお祭り情報や慣習については馴染みが無くても、日本語自体は読解できるので、記事の内容は理解する事ができます。土着の固有名詞について聞かれても、渡された地方新聞に書かれていれば適切な回答をする事ができます。このように、外部情報を与える事によって、より正確で幅広い質問に対応する事が可能になります。
          
          <div class="figure">
          ![](img/01_RAG.jpg){height=200 }
          </div>
          
          外部情報を生成AIが読み出しやすいように離散化して高次元ベクトルとすれば（Embedding）、学習時に含まれていない知識についても回答する事ができるようになります。あらかじめ、社内のデータをEmbeddingし、ベクトルデータベース（ベクトルDB）に格納して生成AIがアクセスできるようにしておけば、その会社独自の生成AIを構築する事も可能です。このような仕組みをRAG（Retrieval Augmented Generation）と呼びます。
          
          #### ③ 生成物が信用できない
          <span class="hashtag">#ハルシネーション</span> 
          <span class="hashtag">#プロンプトエンジニアリング</span> 
          
          先ほど紹介したRAGという手法を用いれば、信州の「おたや祭り」についても正確な情報を言語モデルに生成させる事が可能です。一方、無料版のChatGPTで同様に質問してみると、次のようなデタラメな回答が得られてしましました<span class="notetext">無償版ChatGPTでは、インターネット上の外部情報にアクセスする手段が用意されていないので、モデル学習時に利用したデータのみで回答を生成しようとします。有償版ChatGPT（GPT-4）や、Google Bard、Microsoft Bingなどはインターネット検索機能が具備されているため、より正確な回答が得られるはずです。
          </span>。
          
          <div class="figure">
          ![](img/01_hallucination.png){height=180 }
          </div>
          
          このように、言語モデルが事実に基づかない情報を生成する現象はハルシネーション（Hallucination）と呼ばれます。それらしい情報や答えを出力するため、人間でも真実かどうかを見抜く事が困難な場合もあります。ハルシネーションの原因としては、言語モデルの特性上、質問文に対して尤もらしい文章を生成するように学習されているため、正確さよりも文脈重視となりがちという点が挙げられます。換言すると、ユーザーが求める回答を提供しようと、半ば無理やりに文章を生成してしまうためと言えます。
          
          ハルシネーションを完全に防ぐ事は困難ですが、例えば「自信が無ければ、知らないと回答してください。」という一文をプロンプトの最後に書き添えるだけで、「知りません」という旨の素直な回答を引き出す事も可能です。このように、プロンプトの工夫で意図せぬ回答を防いだり、所望の回答に近づけさせる操作をプロンプトエンジニアリングと呼びます。
          
           また、言語モデルは回答を生成する際に、生成文章の多様性を制御しています。ある質問に対して回答がされるとtemperatureが高いほど、生成されるテキストはより多様で、予測が難しくなります。逆に、temperatureが低いほど、生成されるテキストはより一貫性があり、予測が容易になります。
          
          具体的には、言語モデルは、次に来る単語を予測するために確率分布を使用しています。この確率分布は一般的にはピークがある（一部の単語のみが確率が高くなる）形状をしています。temperatureパラメータはこの確率分布を調整しています。 -->
        </section>
        <section class="level3" aria-labelledby="各章のつながり">
          <h3 id="h3_6">各章のつながり</h3>
          <p>ここまで読み進めていた中でも、馴染みが無い単語や概念があったかもしれません。そもそも機械学習モデルとは何なのか、AIがどのように学習を行うかについて、本書では非エンジニアでも直感的に理解しやすいように記述します。</p>
          <div class="figure">
            
            <img src="img/01_chapter_map.jpg" height="250">
          
          <p>図X-X 各章のつながり</p></div>
          <p>
            図X-Xに示すように、まずは基本知識としてTransformerのイメージを掴む上で基本となる機械学習モデルについて第2章で説明します。ここでは、数字を予測する単純な線形モデル、ゼロ/イチを予測するロジスティク回帰モデル、ニューラルネットワークについて扱います。
            これらの「数字を扱う」モデルについて理解を深めると、「自然言語や画像を扱う」にはどうすればいいのか、という疑問が生じる事かと思います。どのように離散化を行って潜在表現を得るかという点について、自然言語については第３章、画像については第６章で、それぞれ解説します。文章や画像をモデルに投入するイメージが掴めたところで、文章や画像データを解析する実例として分類問題を第４章と第７章でそれぞれ紹介し、実際にPythonというプログラム上で実行して理解を深めるパートを用意しています。最後に、得られた潜在表現から文章や画像を生成する技術について、第５章と第８章で説明します。
            本書を読了する事で、文章生成AIと、画像生成AIに対する理解を深めていきましょう。
          </p>
        </section>
      </section>
    </section>
  

</body></html>