<!DOCTYPE html><html lang="ja"><head>
    <meta charset="utf-8">
    <title></title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="_css/main.css">
  </head>
  <body>
    <section class="level1" aria-labelledby="h3_1">
      <div class="coverpage"><h1 id="h1_0" class="chapter">3章<br>自然言語処理入門</h1></div>
      <div class="subtitle"></div>
      <section class="level2" aria-labelledby="h3_1-1">
        <div class="secheader"><h2 id="h2_0" class="section"><br>自然言語処理で何ができるのか？</h2><br></div>
        <p>
          前章では、AIや機械学習の仕組みについて理解を深めました。本章は生成AIへの序章として、自然言語処理について紹介します。自然言語処理はNLP（Natural Language Processing）とも呼ばれ、我々が使う自然言語（日本語や英語といった話し言葉）で書かれた文章をAIが読み取り、実行する処理です。自然言語処理の技術は、テキストの解析から意味の理解、さらには生成まで、幅広い領域にわたっています。この分野の発展には、統計学的手法や機械学習、最近では深層学習が大きく貢献しています。自然言語を処理することにより、AIは質問に答えたり、文章を要約したり、さらには人間の言葉で新しい文章を生成することが可能になります。
          自然言語処理(NLP)は大きく自然言語理解(NLU)と自然言語生成(NLG)に大別されます。
        </p>
        <ul>
          <li>自然言語理解（NLU：Natural language understanding）
            <ul>
              <li>自然言語理解は、AIが人間の言葉を理解し、その意味や文脈を把握するプロセスです。形態素解析、構文解析、文脈解析などを行って文章の内容を理解する処理であって、文章中の意図や情報を正確に解釈することを目的とします。NLUのアプリケーション例としては、文章分類、文脈認識、感情分析、情報抽出などがあります。ユーザーが入力した自然言語のプロンプト（クエリを含む）に基づいて正確な情報を提供したり、テキストから特定の情報を抽出したりするのに役立ちます。</li>
              <li>Google検索エンジンのアルゴリズムに使用されるようになったBERTが有名です</li>
            </ul>
          </li>
          <li>自然言語生成（NLG：Natural Language Generation）
            <ul>
              <li>自然言語生成は、入力された文章の続きを生成したり文章の変換を行う処理であって、AIが自然言語で新しいテキストを作り出すプロセスです。NLGの用途として、機械翻訳、要約文生成、AIによる創作活動（例えば、物語や詩の作成）などがあります。</li>
              <li>OpenAIのGPTなどが有名です</li>
            </ul>
          </li>
        </ul>
        <p>簡単にまとめると、文章から文脈や意味合いを抽出する処理がNLUであり、文脈や意味合いから文章を生成する処理がNLGとなります（図X-X）。</p>
        <div class="figure">
          
          <img src="img/03_NLP%E3%81%AE%E6%A6%82%E5%BF%B5.jpg" height="200">
        
        <p>図X-X 自然言語処理</p></div>
        <section class="level3" aria-labelledby="自然言語理解nluの使いどころ">
          <h3 id="h3_0">自然言語理解（NLU）の使いどころ</h3>
          <section class="level4" aria-labelledby="音声認識">
            <h4 id="h4_0">音声認識</h4>
            <p>スマートフォンやスマートスピーカーなどのデバイスでは、NLU技術を用いてユーザーの発話からコマンドを認識し、音楽の再生、ニュースの読み上げ、天気予報の提供、スマートホームデバイスの制御などを行います。多少の表現の揺らぎがあっても、ユーザーが実行したい処理を意図通りに実行したり、知りたい内容に対して回答する事が可能になります。</p>
          </section>
          <section class="level4" aria-labelledby="情報抽出と知識管理">
            <h4 id="h4_1">情報抽出と知識管理</h4>
            <p>NLU技術は、大量の文書やテキストデータから重要な情報を抽出し、構造化するのにも用いられます。インターネット検索はもちろん、企業において契約書、報告書、学術論文などから必要な情報を迅速に見つけ出し、知識の管理と活用を効率化が可能となります。</p>
          </section>
          <section class="level4" aria-labelledby="感情分析">
            <h4 id="h4_2">感情分析</h4>
            <p>NLUの応用例として、感情分析があります。これは、文章に含まれる感情や意見を識別する技術です。ソーシャルメディアの投稿、製品レビュー、顧客フィードバックなどから、ポジティブ、ネガティブ、ニュートラルなどの感情を自動で判定します。企業は感情分析を活用して市場のトレンドを把握したり、顧客満足度を評価することが可能となります。</p>
          </section>
        </section>
        <section class="level3" aria-labelledby="自然言語生成nlgの使いどころ">
          <h3 id="h3_1">自然言語生成（NLG）の使いどころ</h3>
          <section class="level4" aria-labelledby="機械翻訳mt-machine-translation">
            <h4 id="h4_3">機械翻訳（MT: Machine Translation）</h4>
            <p>ある文章に対して自然言語処理を行い、別の言語に変換するものです。生成AI技術は翻訳ツールとして開発されたものではありませんが、ある文章を特徴量空間に埋め込み、別の言語として取り出す<span class="notetext" style="font-weight:normal;">特徴量空間に埋め込む話は第一章でも紹介しました。</span>という所作を行うことによって、翻訳でも高い能力を発揮します<span class="notetext" style="font-weight:normal;">例えば2023年に公開された論文（<a href="https://doi.org/10.48550/arXiv.2301.08745%EF%BC%89%E3%82%92%E7%B4%B9%E4%BB%8B%E3%81%97%E3%81%BE%E3%81%99%E3%80%82%E3%81%93%E3%81%AE%E8%AB%96%E6%96%87%E3%81%AF2023%E5%B9%B41%E6%9C%88%E3%81%AB%E7%AC%AC1%E7%89%88%E3%81%8C%E5%85%AC%E9%96%8B%E3%81%95%E3%82%8C%E3%80%81%E3%81%9D%E3%81%AE%E9%9A%9B%E3%81%AB%E3%81%AF%E3%80%8C%E3%83%87%E3%83%BC%E3%82%BF%E3%81%8C%E8%B1%8A%E5%AF%8C%E3%81%AA%E3%83%A8%E3%83%BC%E3%83%AD%E3%83%83%E3%83%91%E8%A8%80%E8%AA%9E%E7%AD%89%E3%81%AB%E3%81%8A%E3%81%84%E3%81%A6%E3%81%AF%E5%95%86%E6%A5%AD%E7%BF%BB%E8%A8%B3%E8%A3%BD%E5%93%81%EF%BC%88Google%E7%BF%BB%E8%A8%B3%E3%81%AA%E3%81%A9%EF%BC%89%E3%81%A8%E7%AB%B6%E5%90%88%E3%81%99%E3%82%8B%E6%80%A7%E8%83%BD%E3%82%92%E7%99%BA%E6%8F%AE%E3%81%99%E3%82%8B%E4%B8%80%E6%96%B9%E3%81%A7%E3%80%81%E3%83%87%E3%83%BC%E3%82%BF%E3%81%8C%E5%B0%91%E3%81%AA%E3%81%84%E8%A8%80%E8%AA%9E%E3%82%84%E9%81%A0%E3%81%84%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AF%E5%A4%A7%E5%B9%85%E3%81%AB%E9%81%85%E3%82%8C%E3%82%92%E3%81%A8%E3%81%A3%E3%81%A6%E3%81%84%E3%82%8B%E3%80%8D%E6%97%A8%E3%81%AE%E7%B5%90%E8%AB%96%E3%81%A7%E3%81%97%E3%81%9F%E3%80%82%E3%81%9D%E3%81%AE%E5%BE%8C%E3%81%ABChatGPT-4%E3%81%8C%E5%85%AC%E9%96%8B%E3%81%95%E3%82%8C%E3%81%9F%E3%81%93%E3%81%A8%E3%81%AB%E3%82%88%E3%81%A3%E3%81%A6%E8%AB%96%E6%96%87%E3%82%82%E6%9B%B4%E6%94%B9%E3%81%95%E3%82%8C%E3%80%812023%E5%B9%B411%E6%9C%88%E3%81%AB%E5%85%AC%E9%96%8B%E3%81%95%E3%82%8C%E3%81%9F%E7%AC%AC4%E7%89%88%E3%81%A7%E3%81%AF%E3%80%8CGPT-4%E3%81%A7%E3%81%AF%E7%BF%BB%E8%A8%B3%E6%80%A7%E8%83%BD%E3%81%AF%E5%A4%A7%E5%B9%85%E3%81%AB%E5%90%91%E4%B8%8A%E3%81%97%E3%80%81%E9%81%A0%E3%81%84%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%82%E3%81%A3%E3%81%A6%E3%82%82%E5%95%86%E6%A5%AD%E7%BF%BB%E8%A8%B3%E8%A3%BD%E5%93%81%E3%81%A8%E9%81%9C%E8%89%B2%E3%81%AA%E3%81%84%E6%80%A7%E8%83%BD%E3%81%A7%E3%81%82%E3%82%8B%E3%80%8D%E6%97%A8%E3%81%AE%E7%B5%90%E8%AB%96%E3%81%8C%E8%BF%B0%E3%81%B9%E3%82%89%E3%82%8C%E3%81%A6%E3%81%84%E3%81%BE%E3%81%99%E3%80%82%E3%81%AA%E3%81%8A%E3%80%81%E5%BD%93%E8%A9%B2%E8%AB%96%E6%96%87%E3%81%AB%E3%81%8A%E3%81%91%E3%82%8B%E7%BF%BB%E8%A8%B3%E6%80%A7%E8%83%BD%E3%81%AE%E8%A9%95%E4%BE%A1%E3%81%AB%E3%81%AFBLEU%E3%82%B9%E3%82%B3%E3%82%A2%E3%81%8C%E4%BD%BF%E7%94%A8%E3%81%95%E3%82%8C%E3%81%A6%E3%81%84%E3%81%BE%E3%81%99%E3%80%82BLEU%E3%82%B9%E3%82%B3%E3%82%A2%E3%81%AF%E6%A9%9F%E6%A2%B0%E7%BF%BB%E8%A8%B3%E7%B5%90%E6%9E%9C%E3%81%AE%E7%B2%BE%E5%BA%A6%E8%A9%95%E4%BE%A1%E6%8C%87%E6%A8%99%E3%81%A8%E3%81%97%E3%81%A6%E5%BA%83%E3%81%8F%E5%88%A9%E7%94%A8%E3%81%95%E3%82%8C%E3%81%A6%E3%81%8A%E3%82%8A%E3%80%81%E8%87%AA%E5%8B%95%E7%BF%BB%E8%A8%B3%E3%81%A8%E4%BA%BA%E9%96%93%E3%81%8C%E4%BD%9C%E6%88%90%E3%81%97%E3%81%9F%E5%8F%82%E8%80%83%E7%BF%BB%E8%A8%B3%E3%81%A8%E3%81%AE%E9%81%95%E3%82%92%E6%B8%AC%E5%AE%9A%E3%81%99%E3%82%8B%E3%82%82%E3%81%AE%E3%81%A7%E3%81%99%E3%80%82%E5%8F%82%E8%80%83%E7%BF%BB%E8%A8%B3%E6%96%87%E3%81%A8%E5%90%8C%E4%B8%80%E3%81%AE%E5%8D%98%E8%AA%9E%E5%88%97%E3%82%92%E5%90%AB%E3%82%80%E5%87%BA%E5%8A%9B%E6%96%87%E3%81%8C%E9%AB%98%E3%81%84%E3%82%B9%E3%82%B3%E3%82%A2%E3%82%92%E7%AE%97%E5%87%BA%E3%81%99%E3%82%8B%E3%81%9F%E3%82%81%E3%80%81%E5%87%BA%E5%8A%9B%E6%96%87%E3%81%AB%E3%81%8A%E3%81%84%E3%81%A6%E6%96%87%E6%B3%95%E7%9A%84%E3%81%AA%E8%AA%A4%E3%82%8A%E3%81%8C%E5%AD%98%E5%9C%A8%E3%81%97%E3%81%A6%E3%82%82%E9%AB%98%E3%81%84%E3%82%B9%E3%82%B3%E3%82%A2%E3%82%92%E7%AE%97%E5%87%BA%E3%81%97%E3%81%A6%E3%81%97%E3%81%BE%E3%81%86%E3%81%A8%E3%81%84%E3%81%86%E6%AC%A0%E7%82%B9%E3%82%82%E3%81%82%E3%82%8A%E3%80%81%E4%BA%BA%E9%96%93%E3%81%8C%E7%BF%BB%E8%A8%B3%E6%96%87%E3%82%92%E8%A9%95%E4%BE%A1%E3%81%97%E3%81%9F%E5%A0%B4%E5%90%88%E3%81%AB%E3%81%AF%E7%95%B0%E3%81%AA%E3%82%8B%E3%82%B9%E3%82%B3%E3%82%A2%E3%81%8C%E7%AE%97%E5%87%BA%E3%81%95%E3%82%8C%E3%82%8B%E5%A0%B4%E5%90%88%E3%81%8C%E3%81%82%E3%82%8A%E3%81%BE%E3%81%99%E3%80%82GPT4%E3%81%8C%E4%BA%BA%E9%96%93%E3%81%AE%E7%BF%BB%E8%A8%B3%E8%80%85%E3%82%92%E5%AE%8C%E5%85%A8%E3%81%AB%E7%BD%AE%E6%8F%9B%E3%81%A7%E3%81%8D%E5%BE%97%E3%82%8B%E3%81%8B%E3%81%A8%E3%81%84%E3%81%86%E7%82%B9%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6%E3%81%AF%E6%B3%A8%E6%84%8F%E3%81%97%E3%81%A6%E8%80%83%E3%81%88%E3%82%8B%E5%BF%85%E8%A6%81%E3%81%8C%E3%81%82%E3%82%8B%E3%81%A7%E3%81%97%E3%82%87%E3%81%86%E3%80%82">https://doi.org/10.48550/arXiv.2301.08745）を紹介します。この論文は2023年1月に第1版が公開され、その際には「データが豊富なヨーロッパ言語等においては商業翻訳製品（Google翻訳など）と競合する性能を発揮する一方で、データが少ない言語や遠い言語では大幅に遅れをとっている」旨の結論でした。その後にChatGPT-4が公開されたことによって論文も更改され、2023年11月に公開された第4版では「GPT-4では翻訳性能は大幅に向上し、遠い言語であっても商業翻訳製品と遜色ない性能である」旨の結論が述べられています。なお、当該論文における翻訳性能の評価にはBLEUスコアが使用されています。BLEUスコアは機械翻訳結果の精度評価指標として広く利用されており、自動翻訳と人間が作成した参考翻訳との違を測定するものです。参考翻訳文と同一の単語列を含む出力文が高いスコアを算出するため、出力文において文法的な誤りが存在しても高いスコアを算出してしまうという欠点もあり、人間が翻訳文を評価した場合には異なるスコアが算出される場合があります。GPT4が人間の翻訳者を完全に置換でき得るかという点については注意して考える必要があるでしょう。</a></span>。</p>
          </section>
          <section class="level4" aria-labelledby="コンテンツ生成">
            <h4 id="h4_4">コンテンツ生成</h4>
            <p>特定のデータに基づき、人間にとって読みやすい概要を自動で作成する応用例です。近年ではGoogle検索結果に生成AIによる検索体験 (SGE: Search Generative Experience) が追加されています。同社は、目的の情報をすばやく簡単に見つける新たな方法と位置づけています<span class="notetext" style="font-weight:normal;">同機能は2023年8月から日本国内でも試験提供が開始されました（<a href="https://japan.googleblog.com/2023/08/search-sge.html%EF%BC%89%E3%80%82">https://japan.googleblog.com/2023/08/search-sge.html）。</a></span>。</p>
            <div class="figure">
              
              <img src="img/03_%E4%BA%BA%E5%89%8D%E3%81%A7%E3%81%86%E3%81%BE%E3%81%8F%E8%A9%B1%E3%81%99%E3%82%B3%E3%83%84.png" height="180">
            
            <p>図X-X SGEの例</p></div>
          </section>
          <section class="level4" aria-labelledby="チャットボットと仮想アシスタント">
            <h4 id="h4_5">チャットボットと仮想アシスタント</h4>
            <p>
              チャットボットについてはChatGPTを想像するとよいかもしれません。自然言語生成（NLG）技術を利用して、人間との会話をシミュレートする応用例です。
              世界で1億人以上のユーザーを有する言語学習サービス「Duolingo」は2023年にOpenAI社のGPT-4を採用し、言語学習者が対話型AIチャットボットを通して会話力を鍛える機能を提供しています。学習者はチャットボットからフィードバックや励ましの言葉を受け取ることもできます<span class="notetext" style="font-weight:normal;">同機能は2023年3月に「Duolingo Max」として公開されました（<a href="https://blog.duolingo.com/duolingo-max/%EF%BC%89%E3%80%82%E4%B8%80%E6%96%B9%E3%81%A72023%E5%B9%B4%E6%99%82%E7%82%B9%E3%81%A7%E3%81%AF%E4%B8%80%E9%83%A8%E3%81%AE%E8%A8%80%E8%AA%9E%E3%81%AB%E9%99%90%E3%82%8A%E6%8F%90%E4%BE%9B%E3%81%95%E3%82%8C%E3%81%A6%E3%81%8A%E3%82%8A%E3%80%81%E7%B1%B3%E5%9B%BD%E3%82%84%E8%8B%B1%E5%9B%BD%E3%81%AA%E3%81%A9%E3%81%AE%E4%B8%80%E9%83%A8%E5%9C%B0%E5%9F%9F%E3%81%A7iOS%E7%89%88%E3%81%AE%E3%81%BF%E3%81%AE%E6%8F%90%E4%BE%9B%E3%81%A8%E3%81%AA%E3%81%A3%E3%81%A6%E3%81%84%E3%81%BE%E3%81%99%E3%80%82">https://blog.duolingo.com/duolingo-max/）。一方で2023年時点では一部の言語に限り提供されており、米国や英国などの一部地域でiOS版のみの提供となっています。</a></span>。
            </p>
            <div class="figure">
              
              <img src="img/03_DuolingoMax.png" height="180">
            
            <p>図X-X 言語学習チャットボットの例</p></div>
          </section>
        </section>
        <section class="level3" aria-labelledby="どのように機械学習すればいいの">
          <h3 id="h3_2">どのように機械学習すればいいの？</h3>
          <p>自然言語処理は我々の身近なアプリケーションに広く応用されているというイメージを持っていただけたと思います。ここで、第二章で学んだように、あらゆる機械学習モデルは数字で処理を行います。自然言語を何らかのカタチで数字表現に変換する必要があります。次節では、文章を数字表現に変換する技法を紹介します。</p>
        </section>
      </section>
      <section class="level2" aria-labelledby="離散化文章を区切る技術">
        <div class="secheader"><h2 id="h2_1">離散化<br>～文章を区切る技術～</h2><br></div>
        <p>
          第一章で言及した通り、自然言語から成る文章は本質的に連続的で複雑なデータであり、時に長い文脈を持ち、一部を切り出しただけでは意味を成さないこともあります。一方で、機械学習モデルで自然言語を処理する際にはモデルが受け取れる形式で受け渡さなければなりません。具体的に説明すると、例えば過去のビール売上に関連する数字で表された特徴量を扱う場合、最初に「来場者数」「気温」といったデータを構造化<span class="notetext" style="font-weight:normal;">構造化とはデータを整理して体系的に配置するプロセスで、端的に言えばExcelやCSVファイルのように「列」と「行」の概念を持たせることです。構造化データは検索、集計、比較などが容易になり、機械学習モデルへの直接的な入力が可能となります。一方で、日常で目にする契約書、帳簿、提案書といったフォーマット（構造定義）が無いデータは非構造データと区分されます。</span>してモデルに入力すればよい一方、文章データの場合は、連続する文章表現を適当に区切った上で数値表現に変換する必要があります。
          すなわち、固定長の数字データ<span class="notetext" style="font-weight:normal;">機械学習モデルは幾つかの特徴量を基にして推論を行います。例えば「来場者数」と「気温」という2種類の特徴量から「売上」を予測するように学習したモデルに対して、推論段階で「湿度」という新たな特徴量を追加して推論を実行することはできません。様々な長さの文章がありますが、文章の長さに応じてモデルの中身を変えるわけにはいかないので、任意の文字列を所定の長さの数字（固定長の数字データ）に変換します。</span>に変換し、コンピューターが処理しやすい形式に変換（離散的な要素に分割）する必要があります。
        </p>
        <div class="figure">
          
          <img src="img/03_%E6%96%87%E7%AB%A0%E3%82%92%E6%95%B0%E5%80%A4%E8%A1%A8%E7%8F%BE%E3%81%AB.jpg" height="190">
        
        <p>図X-X 文章を離散化するイメージ</p></div><!-- 
        
        -->
        <section class="level3" aria-labelledby="文章を区切る">
          <h3 id="h3_3">文章を区切る</h3>
          <p>
            文章を数字に変換するにあたって、まず文章を適当に区切ることが必要です。文字を適当な単位に区切り、この各単位を特徴量とすれば、先に学んだ機械学習で処理ができそうです。このように適切な単位に分割する事をトークナイズ（tokenize）<span class="notetext" style="font-weight:normal;">日本語では「分かち書き」とも。BERTやGPT等の機械学習モデルでは、このような処理をエンコード（encode）と表現されます。</span>と呼びます。英語の場合は、各単語はスペースで区切られているので機械的に処理することが可能ですが、日本語の場合は明示的な区切り記号がないので工夫が必要です。
            文字ごとに区切ってしまう方法（character分割）もありますが、例えば「生成」という単語はこれ単独で"creation"という意味を持ちますから、「生」（"raw"）と「成」（"consist"ないし"compose"）という二つに分割しない方が、文章中の本来の意味を保持する事が出来そうです。
          </p>
          <div class="figure">
            
            <img src="img/03_%E6%96%87%E7%AB%A0%E3%82%92%E5%8C%BA%E5%88%87%E3%82%8B.png" height="180">
          
          <p>図X-X 文章を区切るイメージ</p></div>
          <section class="level4" aria-labelledby="形態素解析">
            <h4 id="h4_6">形態素解析</h4>
            <p>
              図X-Xの自然文章（日本語）を考えた時に、要素ごとに空白で区切りを入れます。この各要素のことを形態素（morpheme; 言語で意味を持つ最小単位）と呼びます。端的に言えば、語彙や文法をルール化し、「少女」は名詞といった具合で、ルールに基づいて分割を行います<span class="notetext" style="font-weight:normal;">最もシンプルな手法は辞書のようにルールを定義するやり方なのですが、2000年前後からは日本語文書の単語出現頻度を考慮して機械学習を行う形態素解析手法（Mecab、Kuromoji、JUMAN、Chasen、など）が現れました。昨今は深層学習を利用した手法（JUMAN++、など）も提案されています。</span>。
              形態素を品詞単位で区切ると、例えば名詞や形容詞といった特定の品詞だけ抜き出して分析を行ったり、係り受けにより文章を解析することができそうです。係り受けとは「文中の言葉同士の関係性」の事で、係り受け解析とは文中の各要素（単語ないし文節）がどのように関連し合っているかを解析する技術を指します。これは、文を構成する要素間の「係り受け関係」を明らかにすることで、文の意味構造を理解するための重要な手段です。日本語では動詞や助詞などが、文節間の関係性を示す重要な因子となり、これらを解析することで文の全体的な意味を捉えることができます。
              例えば「サングラスをかけた少女が公園を駆け回る」という文章において、「少女が」が「駆け回る」に係る主語であること、「公園を」が「駆け回る」に係る目的語であることなど、各文節がどのように他の文節に関連しているかを解析します。この解析により、文の構造を理解し、より複雑な自然言語処理タスクへの応用が可能となります<span class="notetext" style="font-weight:normal;">図X-X中の係り受け解析には自然言語処理ライブラリ「GiNZA」（ギンザ）を用いました。「GiNZA」は株式会社リクルートのAI研究機関であるMegagon Labsと国立国語研究所との共同研究により生まれた日本語自然言語処理オープンソースライブラリです。先進的な自然言語処理ライブラリ「spaCy」をフレームワークとして使用しており、日本語の形態素解析器「SudachiPy」を使用しています。（<a href="https://www.recruit.co.jp/newsroom/2019/0402_18331.html%EF%BC%89">https://www.recruit.co.jp/newsroom/2019/0402_18331.html）</a></span>。
            </p>
            <div class="figure">
              
              <img src="img/03_%E4%BF%82%E3%82%8A%E5%8F%97%E3%81%91%E8%A7%A3%E6%9E%90.jpg" height="150">
            
            <p>図X-X 係り受け解析</p></div>
          </section>
          <section class="level4" aria-labelledby="サブワード分割">
            <h4 id="h4_7">サブワード分割</h4>
            <p>
              先ほどのように形態素や文節で区切ってしまえば離散化自体は可能そうですが、世の中にある全ての形態素を列挙することは非現実的です<span class="notetext" style="font-weight:normal;">仮にこのようなことが出来たとしても、ほぼ無限の語彙数（vocaburary）が必要となり、計算量が増えてしまいます。</span>。更に、学習時に登場しなかった未知語に対しては上手く対処できません<span class="notetext" style="font-weight:normal;">例えば、「マイナポイント」という語句が辞書に登録されていないと「マイナ」「ポイント」（これは2013年に公開された"unidic-mecab-2.1.2"という辞書を用いた場合の分割例です。マイナポイント事業が開始されたのは2019年でしたので、辞書作成当時は未知語でした）といった具合で、意図せずに分割されてしまうことが考えられます。日本語形態素解析における未知語処理手法として、既知語からの派生ルールを用いる方法などが提案されていたりもします（<a href="https://doi.org/10.5715/jnlp.21.1183%EF%BC%89%E3%80%82">https://doi.org/10.5715/jnlp.21.1183）。</a></span>。
              こういった背景から考案されたのが、単語を更に小さな単位（サブワード）に分割するサブワード分割です。極端な話ですが、英語文章を全てサブワードに分解すると、アルファベット26種になります。ここまで分割すると、全ての英文は26種の語から構成されることになりますので、語彙数（Vocaburary）を圧縮できます。論文1本でも学習させれば、全アルファベットが使用されているでしょうから、基本的に未知語に遭遇することはなくなるでしょう。
              実際には、機械学習によって適切なサブワード単位が決定されます。
            </p>
            <p>形態素解析器により入力文を単語単位に分割し、分割された各単語に対して、サブワード分割手法（BPE<span class="notetext" style="font-weight:normal;"> BPE（Byte Pair Encoding; バイトペア符号化）とは、文字の繋がりを見て、頻出する文字同士を結合させる手法です。要するに、単語内の繋がりやすいアルファベットの組み合わせをサブワードとするものです。日本語においては元々の語彙数がアルファベットよりも段違いに多いので、そのペアを探して登録する事を繰り返すと、語彙数が逆に大くなってしまうことが知られています。</span>、WordPiece<span class="notetext" style="font-weight:normal;">考え方としてはBPEに近いです。WordPieceは最も頻度の高いシンボルペアを選択するのではなく、一度語彙に追加された学習データの尤度を最大化するものを選択します。</span>など）を用いてサブワード単位に分割します。れらの手法は主に教師なし学習が用いられていて、文章データさえあればトークナイザを構築できます。サブワード単位に分割された語を言語モデルへの入力に用いることで、低頻度語に対して頑健性を向上させます。</p>
          </section>
          <section class="level4" aria-labelledby="sentencepiece">
            <h4 id="h4_8">SentencePiece</h4>
            <p>
              一方、それなら最初から形態素に分けずに分割してしまおう、という手法がSentencePieceです。この手法は言語に関する事前知識を必要とせず、生のテキストデータから直接サブワード単位を学習します。英語圏言語で用いられている空白スペースも文字として扱うので、日本語や中国語のようにスペース区切りでは無い言語と同様に扱うことが可能です。
              この手法はGoogle社が開発し、GitHub上でオープンソースとして公開<span class="notetext" style="font-weight:normal;">GitHubはソフトウェア開発のプラットフォームで、個人や企業を問わず幅広いユーザーがコードを共有しています。8,000万件以上ものプロジェクトが管理されており、SentencePieceは次のURLにてApache2.0ライセンス下で公開され、尚も継続的に開発とメンテナンスが行われています（<a href="https://github.com/google/sentencepiece%EF%BC%89%E3%80%82">https://github.com/google/sentencepiece）。</a></span>されているため、研究者や開発者は自由に利用できます。
              SentencePieceは、特にトランスフォーマーベースのモデル（例えば、BERTやGPT）の前処理として使用されています。
            </p>
          </section>
          <section class="level4" aria-labelledby="まとめ">
            <h4 id="h4_9">まとめ</h4>
            <p>形態素による係り受け解析は、助詞や名詞のデータベースに基づく、人が作成したルールベースの処理によって文章を解析します。自然言語の文法的構造を理解する上で重要であると考えられています。このようなルールベースのアプローチは、特定の言語の文法規則や特性を詳細に反映することができるため、高い精度での解析が可能です。一方でこの手法は、ルールの作成とメンテナンスに多大な労力が必要であり、未知の表現や言語の変化に対応するためには定期的な更新が必要となります。</p>
            <p>
              BERTやGPT等の現代的な機械学習モデルでは、Encode後のトークン間の関連性を学習して、人が作成したルールに依存せずに自ら係り受けの学習（に近いこと）をしていると考えることができます。このように自ら特徴を学習するには、Self-Attentionという要素技術が重要なのですが、こちらについてはTransformetに関する説明の章で述べます。
              OpenAIのGPTシリーズでは、tiktokenというトークナイザー<span class="notetext" style="font-weight:normal;">高速BPEトークナイザーで、トークナイザーの挙動はOpenAI社のWebページ上で確認することが出来ます（<a href="https://platform.openai.com/tokenizer%EF%BC%89%E3%80%82">https://platform.openai.com/tokenizer）。</a></span>が使用されています。図X-Xに「こんにちは。今日の天気は？」を分解した様子をしまします。「こんにちは」のように語句は単一のトークンとして処理されていますが、それ以外の文字は単独で区切られています<span class="notetext" style="font-weight:normal;">GPT-4やGPT-3.5では、「cl100k_base」というエンコーディングが使用されています。例えば、 "こんにちは。今日の天気は何ですか？"という文章をトーカナイズすると、次の12トークンに分割されます。
トークン：['こんにちは', '。', '今', '日', 'の', '天', '気', 'は', '何', 'です', 'か', '？']
これらの各トークンにはIDが割り振られており、モデルに入れる際には文字ではなく、これらのIDで識別されることになります。（トークンID： [90115, 1811, 37271, 9080, 16144, 36827, 95221, 15682, 99849, 38641, 32149, 11571]）</span>。
            </p>
            <div class="figure">
              
              <img src="img/03_%E4%BB%8A%E6%97%A5%E3%81%AE%E5%A4%A9%E6%B0%97%E3%81%AF.png" height="250">
            
            <p>図X-X GPTにおける文章の区切り方</p></div>
            <p>
              人間の感覚からすれば、「天気」は単独で取り出した方が意味を成しそうですが、多言語の大量文章を学習した結果、これらは分けた方が都合が良いという結果になったのでしょう。現代的な機械学習モデルで使用しているSelf-Attentionが強力なのは、人が作成したルールに依存せずに、広域的な単語（Encodeした後のトークン単位）間の関連性を学習可能である点です。例えば係り受けを学習するにあたって、人間が作成したルールに基づいて分割された単語間の係り受けを学習する限り人間の性能を超えることは難しいでしょう。ルールベースの形態素解析ではなく、更に細かいトークン単位での学習を行う大きな理由です。
              Self-Attentionメカニズムを採用した現代的な大規模言語モデルは、単語やフレーズの分割方法に対する人間の直感的な理解を超える能力を持っています。これらのモデルは、大規模なデータセットから複雑な言語パターンを学習することで、より洗練された言語理解と処理を実現します。この進歩は、人間が作成したルールベースのシステムでは到達できない新たな可能性を開くものであり、自然言語理解の分野における今後の研究や応用に大きな影響を与え続けるでしょう。
            </p>
          </section>
        </section>
        <section class="level3" aria-labelledby="参考前処理手法">
          <h3 id="h3_4">参考：前処理手法</h3>
          <p>ここでは、参考として文章を離散化する際に用いられる前処理手法を紹介します。対象となる文章に合わせて、複数の手法を組み合わせて適切な分析を行います。</p>
          <section class="level4" aria-labelledby="正規化表記揺れを是正">
            <h4 id="h4_10">正規化（表記揺れを是正）</h4>
            <p>文字種の統一、つづりや表記揺れの吸収といった単語を置換して統一する処理をします。後続の処理における計算量やメモリ使用量の観点から見ても重要な処理です。</p>
            <ul>
              <li>文字種の統一
                <ul>
                  <li>文字種の統一ではアルファベットの大文字を小文字に変換する、半角文字を全角文字に変換するといった処理を行います。たとえば「Cat」の大文字部分を小文字に変換して「cat」にしたり、「ﾈｺ」を全角に変換して「ネコ」にします。このような処理をすることで、単語を文字種の区別なく同一の単語として扱えるようになります。</li>
                </ul>
              </li>
              <li>数字の置き換え
                <ul>
                  <li>数字の置き換えでは文章中に出現する数字を別の記号(たとえば0)に置き換えます。たとえば、ある文章中に「2017年1月1日」のような数字が含まれる文字列が出現したとしましょう。数字の置き換えではこの文字列中の数字を「0年0月0日」のように変換してしまいます。数値表現が多様で出現頻度が高い割には自然言語処理のタスクに役に立たないことが多いからです。</li>
                </ul>
              </li>
              <li>辞書を用いた単語の統一
                <ul>
                  <li>辞書<span class="notetext" style="font-weight:normal;">有名な辞書としてWordNetが挙げられます。単語間の意味的関係を体系的に整理した大規模な辞書データベースで、単語を「シノニムセット（synsets）」と呼ばれる同義語のグループに分類し、これらのグループ間の様々な意味関係（例えば、上位語/下位語関係、部分/全体関係など）を定義しています。異なる単語が同じ概念を指している場合や、同じ単語が複数の意味を持つ場合、WordNetを使用してこれらの単語や意味の関係を特定し、文脈に応じて表記ゆれを是正することが可能です。</span>がを用いた単語の統一では単語を代表的な表現に置き換えます。たとえば、「オレンジ」と「orange」という表記が入り混じった文章を扱う時に、どちらかの表現に寄せて置き換えてしまいます。これにより、これ以降の処理で2種類の単語を同じ単語として扱えるようになります。置き換える際には文脈を考慮して置き換える必要があることには注意する必要があります。</li>
                </ul>
              </li>
            </ul>
          </section>
          <section class="level4" aria-labelledby="ストップワードstop-word">
            <h4 id="h4_11">ストップワード（stop word）</h4>
            <p>
              ストップワードとは、自然言語処理を実行する際に一般的で役に立たない等の理由で処理対象外とする単語のことです。たとえば、助詞や助動詞などの機能語(「は」「の」「です」「ます」など)が挙げられます<span class="notetext" style="font-weight:normal;">英語のストップワードとしては、"the"、"a/an"、"is"、"at"、"which"、"on" などの前置詞、冠詞、接続詞が挙げられます</span>。これらの単語は出現頻度が高い割に役に立たず、計算量や性能に悪影響を及ぼすため除去されます。
              ストップワードの除去には様々な方式がありますが、この記事では以下の2つの方式を紹介します。
            </p>
            <ul>
              <li>辞書による方式
                <ul>
                  <li>この方式では、あらかじめ定義されたストップワードのリスト（辞書）を使用して文章からストップワードを除去します。この辞書は、その言語で一般的に使用される前置詞、冠詞、接続詞などの単語を含んでおり、自然言語処理においてほとんど意味を持たないと考えられる単語から構成されます。辞書を用いる利点として、シンプルで実装が容易である点が挙げられます。一方で、辞書が固定されているため、特定のドメインやコンテキストに特有のストップワードをカバーしきれない可能性があります。</li>
                </ul>
              </li>
              <li>出現頻度による方式
                <ul>
                  <li>この方式では、テキスト全体での単語の出現頻度を分析し、あまりにも頻繁に出現する単語や、逆にほとんど出現しない単語をストップワードとして扱います。頻繁に出現する単語は、一般的に内容の理解にあまり寄与しないと考えられるため、ストップワードとして除外されます（例えば、「の」「あの」「この」など）。一方、非常に稀にしか出現しない単語も重要な意味を持たない場合が多いため、除去の対象となることがあります。与えられたデータに対して柔軟にストップワードを定義できますが、単語の出現頻度だけを基準にして機械的に除去してしまうと、重要な情報を持つ可能性のある単語を除去してしまうリスクも伴います。</li>
                </ul>
              </li>
            </ul>
          </section>
          <section class="level4" aria-labelledby="まとめ-1">
            <h4 id="h4_12">まとめ</h4>
            <p>自然言語で書かれた文章の前処理では、正規化やストップワードの除去といったステップを適切に行うことが、分析精度を大きく左右します。文章データから、分析に不要な表現の揺らぎを取り除き、データを機械学習モデルやその他の分析手法で扱いやすい形に整形するために不可欠です。どのような前処理手法を適用するか、またその程度は、次ような観点に基づいて慎重に選択する必要があります。</p>
            <ul>
              <li>文章データの性質
                <ul>
                  <li>文章で扱われている言語や専門分野によって、適切な前処理手法が異なります。例えば、技術文書や医療記録などの特定のドメインでは、稀有な専門用語であっても重要な意味を持つと考えられるため、これらをストップワードとして除外すべきではないでしょう。</li>
                  <li>SNSの投稿、ニュース記事、学術論文など、テキストの形式や出典によって、言語の使用法やスタイルが異なるため、前処理のアプローチも変わってきます。例えば、絵文字や顔文字は、その感情的な表現に意味があったり、何かを代替して表現する場合があるため、どのように処理を行うかは検討に値します。</li>
                </ul>
              </li>
              <li>分析の目的
                <ul>
                  <li>タスク: 分析の目的に応じて、特定の単語やフレーズが重要になったり、逆に無視されたりします。例えば、感情分析では否定語が重要な役割を果たすため、これらを除外しないようにする必要があります。</li>
                  <li>モデルの要件: 使用する機械学習モデルやアルゴリズムによって、データの形式やどの情報が重要であるかが異なります。深層学習モデルでは生のテキストデータをそのまま利用できることもありますが、統計的モデルではより細かい前処理が必要になる場合があります。</li>
                </ul>
              </li>
            </ul>
          </section>
        </section>
      </section>
      <section class="level2" aria-labelledby="単語文章行列bow-tf-idf">
        <div class="secheader"><h2 id="h2_2"><br>単語文章行列（BoW, TF-IDF）</h2><br></div>
        <p>
          前節では文章をトークンに分割するイメージについて理解を深めました。一方でこのままでは尚も数字ではないため、数値的に解析可能な形式に変換するため（モデルに入力するため）には何らかの処理が必要そうです。
          各トークンに対して、例えば文書に出現する単語の頻度を数値化すれば、各トークンの特徴を表現できそうです。このように作成したデータを単語文書行列と言い、文章を数値化する上で基本的な手法となります。
        </p>
        <section class="level3" aria-labelledby="bag-of-words-bow">
          <h3 id="h3_5">Bag of Words (BoW)</h3>
          <p>BoWモデルは、テキスト内の単語の出現回数をカウントし、それを特徴ベクトルとして表現します。このとき、単語の順序や文脈は無視され、単純に単語の「袋」として扱われます。例えば、二つの文書があった場合、BoWモデルは各文書に含まれる全ての単語のリストを作成し、各単語がその文書に何回出現するかをカウントします。この方法により、文書を固定長のベクトルとして表現することができ、テキスト分類や検索、さらには感情分析など、多岐にわたるアプリケーションで利用されます。</p>
        </section>
        <section class="level3" aria-labelledby="tf-idf">
          <h3 id="h3_6">TF-IDF</h3>
          <p>TF-IDFは、BoWモデルをさらに洗練させた手法で、単語の出現頻度だけでなく、その単語がどれだけ「重要」であるかを加味した重み付けを行います。具体的には、TF（term frequency）は単語が文書内にどれだけ頻繁に出現するかを示し、IDF（inverse document frequency）はその単語が文書集合全体の中でどれだけ珍しいかを示します。これにより、一般的な単語（例えば「は」「が」「と」など）が高い頻度で出現しても、その重要度は低く評価され、特定のトピックに特化した単語が強調されます。TF-IDFは、情報検索や文書分類、文章の類似度計算などに広く用いられています。</p>
          <p>これらの手法により、テキストデータから有用な特徴を抽出し、機械学習モデルの入力として利用することができます。BoWとTf-idfは、テキストデータを扱う上での基礎であり、これらの概念を理解することは、生成AIを含む多くの自然言語処理アプリケーションの背後にあるメカニズムを理解する上で欠かせません。</p>
        </section>
      </section>
      <section class="level2" aria-labelledby="word2vecskip-gram-cbow">
        <div class="secheader"><h2 id="h2_3">word2vec（Skip-gram, CBOW）</h2></div>
      </section>
    </section>
  

</body></html>