---
link:
  - rel: 'stylesheet'
    href: '_css/main.css'
lang: 'ja'
---


# 3章<br>自然言語処理入門{#h3_1 .chapter}
<div class="subtitle">
</div>

## <br>自然言語処理で何ができるのか？{#h3_1 .section}
<br>

前章では、AIや機械学習の仕組みについて理解を深めました。本章は生成AIへの序章として、自然言語処理について紹介します。自然言語処理はNLP（Natural Language Processing）とも呼ばれ、我々が使う自然言語（日本語や英語といった話し言葉）で書かれた文章をAIが読み取り、実行する処理です。自然言語処理の技術は、テキストの解析から意味の理解、さらには生成まで、幅広い領域にわたっています。この分野の発展には、統計学的手法や機械学習、最近では深層学習が大きく貢献しています。自然言語を処理することにより、AIは質問に答えたり、文章を要約したり、さらには人間の言葉で新しい文章を生成することが可能になります。
自然言語処理(NLP)は大きく自然言語理解(NLU)と自然言語生成(NLG)に大別されます。

- 自然言語理解（NLU：Natural language understanding）
  - 自然言語理解は、AIが人間の言葉を理解し、その意味や文脈を把握するプロセスです。形態素解析、構文解析、文脈解析などを行って文章の内容を理解する処理であって、文章中の意図や情報を正確に解釈することを目的とします。NLUのアプリケーション例としては、文章分類、文脈認識、感情分析、情報抽出などがあります。ユーザーが入力した自然言語のプロンプト（クエリを含む）に基づいて正確な情報を提供したり、テキストから特定の情報を抽出したりするのに役立ちます。
  - Google検索エンジンのアルゴリズムに使用されるようになったBERTが有名です
- 自然言語生成（NLG：Natural Language Generation）
  - 自然言語生成は、入力された文章の続きを生成したり文章の変換を行う処理であって、AIが自然言語で新しいテキストを作り出すプロセスです。NLGの用途として、機械翻訳、要約文生成、AIによる創作活動（例えば、物語や詩の作成）などがあります。
  - OpenAIのGPTなどが有名です

簡単にまとめると、文章から文脈や意味合いを抽出する処理がNLUであり、文脈や意味合いから文章を生成する処理がNLGとなります（図X-X）。

@div:figure
![](img/03_NLPの概念.jpg){height=200}
<p>
図X-X 自然言語処理
</p>
@divend

### 自然言語理解（NLU）の使いどころ
#### 音声認識
スマートフォンやスマートスピーカーなどのデバイスでは、NLU技術を用いてユーザーの発話からコマンドを認識し、音楽の再生、ニュースの読み上げ、天気予報の提供、スマートホームデバイスの制御などを行います。多少の表現の揺らぎがあっても、ユーザーが実行したい処理を意図通りに実行したり、知りたい内容に対して回答する事が可能になります。

#### 情報抽出と知識管理
NLU技術は、大量の文書やテキストデータから重要な情報を抽出し、構造化するのにも用いられます。インターネット検索はもちろん、企業において契約書、報告書、学術論文などから必要な情報を迅速に見つけ出し、知識の管理と活用を効率化が可能となります。

#### 感情分析
NLUの応用例として、感情分析があります。これは、文章に含まれる感情や意見を識別する技術です。ソーシャルメディアの投稿、製品レビュー、顧客フィードバックなどから、ポジティブ、ネガティブ、ニュートラルなどの感情を自動で判定します。企業は感情分析を活用して市場のトレンドを把握したり、顧客満足度を評価することが可能となります。

### 自然言語生成（NLG）の使いどころ
#### 機械翻訳（MT: Machine Translation）
ある文章に対して自然言語処理を行い、別の言語に変換するものです。生成AI技術は翻訳ツールとして開発されたものではありませんが、ある文章を特徴量空間に埋め込み、別の言語として取り出す<span class="notetext" style="font-weight:normal;">特徴量空間に埋め込む話は第一章でも紹介しました。</span>という所作を行うことによって、翻訳でも高い能力を発揮します<span class="notetext" style="font-weight:normal;">例えば2023年に公開された論文（https://doi.org/10.48550/arXiv.2301.08745）を紹介します。この論文は2023年1月に第1版が公開され、その際には「データが豊富なヨーロッパ言語等においては商業翻訳製品（Google翻訳など）と競合する性能を発揮する一方で、データが少ない言語や遠い言語では大幅に遅れをとっている」旨の結論でした。その後にChatGPT-4が公開されたことによって論文も更改され、2023年11月に公開された第4版では「GPT-4では翻訳性能は大幅に向上し、遠い言語であっても商業翻訳製品と遜色ない性能である」旨の結論が述べられています。なお、当該論文における翻訳性能の評価にはBLEUスコアが使用されています。BLEUスコアは機械翻訳結果の精度評価指標として広く利用されており、自動翻訳と人間が作成した参考翻訳との違を測定するものです。参考翻訳文と同一の単語列を含む出力文が高いスコアを算出するため、出力文において文法的な誤りが存在しても高いスコアを算出してしまうという欠点もあり、人間が翻訳文を評価した場合には異なるスコアが算出される場合があります。GPT4が人間の翻訳者を完全に置換でき得るかという点については注意して考える必要があるでしょう。</span>。

#### コンテンツ生成
特定のデータい基づき、人間にとって読みやすい概要を自動で作成する応用例です。近年ではGoogle検索結果に生成AIによる検索体験 (SGE: Search Generative Experience) が追加されています。同社は、目的の情報をすばやく簡単に見つける新たな方法と位置づけています<span class="notetext" style="font-weight:normal;">同機能は2023年8月から日本国内でも試験提供が開始されました（https://japan.googleblog.com/2023/08/search-sge.html）。</span>。

@div:figure
![](img/03_人前でうまく話すコツ.png){height=180}
<p>
図X-X SGEの例
</p>
@divend

#### チャットボットと仮想アシスタント
チャットボットについてはChatGPTを想像するとよいかもしれません。自然言語生成（NLG）技術を利用して、人間との会話をシミュレートする応用例です。
世界で1億人以上のユーザーを有する言語学習サービス「Duolingo」は2023年にOpenAI社のGPT-4を採用し、言語学習者が対話型AIチャットボットを通して会話力を鍛える機能を提供しています。学習者はチャットボットからフィードバックや励ましの言葉を受け取ることもできます<span class="notetext" style="font-weight:normal;">同機能は2023年3月に「Duolingo Max」として公開されました（https://blog.duolingo.com/duolingo-max/）。一方で2023年時点では一部の言語に限り提供されており、米国や英国などの一部地域でiOS版のみの提供となっています。</span>。

@div:figure
![](img/03_DuolingoMax.png){height=180}
<p>
図X-X 言語学習チャットボットの例
</p>
@divend

### どのように機械学習すればいいの？
自然言語処理は我々の身近なアプリケーションに広く応用されているというイメージを持っていただけたと思います。ここで、第二章で学んだように、あらゆる機械学習モデルは数字で処理を行います。自然言語を何らかのカタチで数字表現に変換する必要があります。次節では、文章を数字表現に変換する技法を紹介します。

## 離散化<br>～文章を区切る技術～
<br>

第一章で言及した通り、自然言語から成る文章は本質的に連続的で複雑なデータであり、時に長い文脈を持ち、一部を切り出しただけでは意味をなさないこともあります。一方で、機械学習モデルで自然言語を処理する際には、モデルが受け取れる形式で受け渡さなければなりません。すなわち、固定長の数字データ<span class="notetext" style="font-weight:normal;">第二章で学んだ通り、機械学習モデルは幾つかの特徴量を基にして推論を行います。例えば「来場者数」と「気温」という2種類の特徴量から「売上」を予測するように学習したモデルに対して、推論段階で「湿度」も追加することはできません。様々な長さの文章がありますが、文章の長さに応じてモデルの中身を変えるわけにはいかないので、任意の文字列を所定の長さの数字（固定長の数字データ）に変換します。</span>に変換し、コンピューターが処理しやすい形式に変換（離散的な要素に分割）する必要があります。

### 文章を区切る
文章を数字に変換するにあたって、まず文章を適当に区切ることが必要です。文字を適当な単位に区切り、この各単位を特徴量とすれば、先に学んだ機械学習で処理ができそうです。このように適切な単位に分割する事をトーカナイズ（Tokenize）<span class="notetext" style="font-weight:normal;">「分かち書き」とも。BERTやGPT等の機械学習モデルでは、このような処理をEncodeと表現されます。</span>と呼びます。英語の場合は、各単語はスペースで区切られているので機械的に処理することが可能ですが、日本語の場合は明示的な区切り記号がないので工夫が必要です。
文字ごとに区切ってしまう方法（character分割）もありますが、例えば「生成」という単語はこれ単独で"Creation"という意味を持ちますから、「生」（"Raw"）と「成」（"Consist"）という二つに分割しない方が、文章中の本来の意味を保持する事が出来そうです。

@div:figure
![](img/03_文章を区切る.png){height=180}
<p>
図X-X 文章を区切るイメージ
</p>
@divend

#### 形態素解析
次の自然文章（日本語）を考えた時、品詞に分解できる事がわかります。この各要素のことを形態素（Morpheme：言語で意味を持つ最小単位）と呼びます。日本語のトーカナイザとして、MeCab、Kuromoji、Jumanなどが知られています。
形態素を品詞単位で区切ると、例えば名詞や形容詞といった特定の品詞だけ抜き出して分析を行ったり、係り受けにより文章を解析することができそうです。

#### サブワード分割
形態素で区切ってしまえば解決しそうですが、世の中にある形態素を列挙することは非現実的<span class="notetext" style="font-weight:normal;">仮にこのようなことが出来たとしても、ほぼ無限の語彙数（Vocaburary）が必要となり、計算量が増えてしまいます。
</span>です。学習時に登場しなかった未知語に対しては、上手く対処できません。
こういった背景から考案されたのが、単語を更に小さな単位（サブワード）に分割しようというサブワード分割です。極端な話ですが、英語文章を全てサブワードに分解すると、アルファベット26種になります。ここまで分割すると、全ての英文は26種の語から構成されることになりますので、語彙数（Vocaburary）を圧縮することができます。論文1本でも学習させれば、全アルファベットが使用されているでしょうから、基本的に未知語に遭遇することはなくなるでしょう。
形態素解析器により入力文を単語単位に分割し、分割された各単語に対して、サブワード分割手法（BPE<span class="notetext" style="font-weight:normal;"> BPE（Byte Pair Encoding）とは、文字の繋がりを見て、頻出する文字同士を結合させる手法です。要するに、単語内の繋がりやすいアルファベットの組み合わせをサブワードとするものです。日本語においては、元々の語彙数がアルファベットよりも段違いに多いので、そのペアを探して登録して、とやってゆくと、語彙数が逆に大きすぎるものになってしまいます。</span>、Wordpiece<span class="notetext" style="font-weight:normal;">考え方としてはBPEに近いです。WordPieceは最も頻度の高いシンボルペアを選択するのではなく、一度語彙に追加された学習データの尤度を最大化するものを選択します。</span>など）を用いてサブワード単位に分割します。サブワード単位に分割された語を言語モデルへの入力に用いることで、低頻度語に対して頑健性を向上させます。

#### SentencePiece
一方、それなら最初から形態素に分けずに分割してしまおう、という手法がSentencePieceです。この手法は言語に関する事前知識を必要とせず、生のテキストデータから直接サブワード単位を学習します。英語圏言語で用いられている空白スペースも文字として扱うので、日本語や中国語のようにスペース区切りでは無い言語と同様に扱うことが可能です。
Googleが開発し、GitHub上でオープンソースとして公開<span class="notetext" style="font-weight:normal;">GitHubはソフトウェア開発のプラットフォームで、個人や企業を問わず幅広いユーザーがコードを共有しています。8000万件以上ものプロジェクトが管理されており、SentencePieceは次のURLでApache2.0ライセンス下で公開され、尚も継続的に開発とメンテナンスが行われています（https://github.com/google/sentencepiece）。</span>されているため、研究者や開発者は自由に利用することができます。
SentencePieceは、特にトランスフォーマーベースのモデル（例えば、BERTやGPT）の前処理として使用されています。

#### まとめ
形態素による係り受け解析は、助詞や名詞のデータベースに基づく、人が作成したルールベースの処理によって文章を解析します。自然言語の文法的構造を理解する上で重要であると考えられています。このようなルールベースのアプローチは、特定の言語の文法規則や特性を詳細に反映することができるため、高い精度での解析が可能です。一方でこの手法は、ルールの作成とメンテナンスに多大な労力が必要であり、未知の表現や言語の変化に対応するためには定期的な更新が必要となります。

BERTやGPT等の現代的な機械学習モデルでは、Encode後のトークン間の関連性を学習して、人が作成したルールに依存せずに自ら係り受けの学習（に近いこと）をしていると考えることができます。このように自ら特徴を学習するには、Self-Attentionという要素技術が重要なのですが、こちらについてはTransformetに関する説明の章で述べます。
OpenAIのGPTシリーズでは、高速BPEトークナイザー「tiktoken」<span class="notetext" style="font-weight:normal;">このトーカナイザーの挙動はOpenAI社のWebページ上で確認することが出来ます（https://platform.openai.com/tokenizer）。</span>が使用されています。図X-Xに「こんにちは。今日の天気は？」を分解した様子をしまします。「こんにちは」のように高頻出語は単一のトークンとして処理されていますが、それ以外の文字は単独で区切られています<span class="notetext" style="font-weight:normal;">GPT-4やGPT-3.5では、「cl100k_base」というエンコーディングが使用されています。例えば、 "こんにちは。今日の天気は何ですか？"という文章をトーカナイズすると、次の12トークンに分割されます。
トークン：['こんにちは', '。', '今', '日', 'の', '天', '気', 'は', '何', 'です', 'か', '？']
これらの各トークンにはIDが割り振られており、モデルに入れる際には文字ではなく、これらのIDで識別されることになります。（トークンID： [90115, 1811, 37271, 9080, 16144, 36827, 95221, 15682, 99849, 38641, 32149, 11571]）</span>。

@div:figure
![](img/03_今日の天気は.png){height=250}
<p>
図X-X GPTにおける文章の区切り方
</p>
@divend

人間の感覚からすれば、「天気」は単独で取り出した方が意味が通じやすくなりそうですが、多言語の大量文章を学習した結果、これらは分けた方が都合が良いという結果になったのでしょう。現代的な機械学習モデルで使用しているSelf-Attentionが強力なのは、人が作成したルールに依存せずに、広域的な単語（Encodeした後のトークン単位）間の関連性を学習出来る点です。例えば係り受けを学習するにあたって、人間が作成したルールに基づいて分割された単語間の係り受けを学習する限り、人間の性能を超えることが出来ない、形態素解析ではなくもっと細かいトークン単位での学習を行う大きな理由です。

### 参考：前処理手法
ここでは、参考として文章を離散化する際に用いられる前処理手法を紹介します。対象となる文章に合わせて、複数の手法を組み合わせて適切な分析を行います。

#### 正規化（表記揺れを是正）
文字種の統一、つづりや表記揺れの吸収といった単語を置き換える処理をします。この処理を行うことで、全角の「ネコ」と半角の「ﾈｺ」やひらがなの「ねこ」を同じ単語として処理できるようになります。後続の処理における計算量やメモリ使用量の観点から見ても重要な処理です。
- 文字種の統一
  - 文字種の統一ではアルファベットの大文字を小文字に変換する、半角文字を全角文字に変換するといった処理を行います。たとえば「Natural」の大文字部分を小文字に変換して「natural」にしたり、「ﾈｺ」を全角に変換して「ネコ」にします。このような処理をすることで、単語を文字種の区別なく同一の単語として扱えるようになります。
- 数字の置き換え
  - 数字の置き換えでは文章中に出現する数字を別の記号(たとえば0)に置き換えます。たとえば、ある文章中に「2017年1月1日」のような数字が含まれる文字列が出現したとしましょう。数字の置き換えではこの文字列中の数字を「0年0月0日」のように変換してしまいます。数値表現が多様で出現頻度が高い割には自然言語処理のタスクに役に立たないことが多いからです。
- 辞書を用いた単語の統一
  - 辞書<span class="notetext" style="font-weight:normal;">有名な辞書としてWordNetが挙げられます。単語間の意味的関係を体系的に整理した大規模な辞書データベースで、単語を「シノニムセット（synsets）」と呼ばれる同義語のグループに分類し、これらのグループ間の様々な意味関係（例えば、上位語/下位語関係、部分/全体関係など）を定義しています。異なる単語が同じ概念を指している場合や、同じ単語が複数の意味を持つ場合、WordNetを使用してこれらの単語や意味の関係を特定し、文脈に応じて表記ゆれを是正することが可能です。</span>がを用いた単語の統一では単語を代表的な表現に置き換えます。たとえば、「オレンジ」と「orange」という表記が入り混じった文章を扱う時に、どちらかの表現に寄せて置き換えてしまいます。これにより、これ以降の処理で2種類の単語を同じ単語として扱えるようになります。置き換える際には文脈を考慮して置き換える必要があることには注意する必要があります。

#### ストップワード（stop word）
ストップワードとは、自然言語処理を実行する際に一般的で役に立たない等の理由で処理対象外とする単語のことです。たとえば、助詞や助動詞などの機能語(「は」「の」「です」「ます」など)が挙げられます<span class="notetext" style="font-weight:normal;">英語のストップワードとしては、"the"、"a/an"、"is"、"at"、"which"、"on" などの前置詞、冠詞、接続詞が挙げられます</span>。これらの単語は出現頻度が高い割に役に立たず、計算量や性能に悪影響を及ぼすため除去されます。
ストップワードの除去には様々な方式がありますが、この記事では以下の2つの方式を紹介します。
- 辞書による方式
  - この方式では、あらかじめ定義されたストップワードのリスト（辞書）を使用して文章からストップワードを除去します。この辞書は、その言語で一般的に使用される前置詞、冠詞、接続詞などの単語を含んでおり、自然言語処理においてほとんど意味を持たないと考えられる単語から構成されます。辞書を用いる利点として、シンプルで実装が容易である点が挙げられます。一方で、辞書が固定されているため、特定のドメインやコンテキストに特有のストップワードをカバーしきれない可能性があります。
- 出現頻度による方式
  - この方式では、テキスト全体での単語の出現頻度を分析し、あまりにも頻繁に出現する単語や、逆にほとんど出現しない単語をストップワードとして扱います。頻繁に出現する単語は、一般的に内容の理解にあまり寄与しないと考えられるため、ストップワードとして除外されます（例えば、「の」「あの」「この」など）。一方、非常に稀にしか出現しない単語も重要な意味を持たない場合が多いため、除去の対象となることがあります。与えられたデータに対して柔軟にストップワードを定義できますが、単語の出現頻度だけを基準にして機械的に除去してしまうと、重要な情報を持つ可能性のある単語を除去してしまうリスクも伴います。

#### まとめ
自然言語で書かれた文章の前処理では、正規化やストップワードの除去といったステップを適切に行うことが、分析精度を大きく左右します。文章データから、分析に不要な表現の揺らぎを取り除き、データを機械学習モデルやその他の分析手法で扱いやすい形に整形するために不可欠です。どのような前処理手法を適用するか、またその程度は、次ような因子に基づいて慎重に選択する必要があります。
- 文章データの性質
  - 文章で扱われている言語や専門分野によって、適切な前処理手法が異なります。例えば、技術文書や医療記録などの特定のドメインでは、稀有な専門用語であっても重要な意味を持つと考えられるため、これらをストップワードとして除外すべきではないでしょう。
  - SNSの投稿、ニュース記事、学術論文など、テキストの形式や出典によって、言語の使用法やスタイルが異なるため、前処理のアプローチも変わってきます。例えば、絵文字や顔文字は、その感情的な表現に意味があったり、何かを代替して表現する場合があるため、どのように処理を行うかは検討に値します。
- 分析の目的
  - タスク: 分析の目的に応じて、特定の単語やフレーズが重要になったり、逆に無視されたりします。例えば、感情分析では否定語が重要な役割を果たすため、これらを除外しないようにする必要があります。
  - モデルの要件: 使用する機械学習モデルやアルゴリズムによって、データの形式やどの情報が重要であるかが異なります。深層学習モデルでは生のテキストデータをそのまま利用できることもありますが、統計的モデルではより細かい前処理が必要になる場合があります。

## 単語文章行列（BoW, Tf-idf）

## word2vec（Skip-gram, CBOW）
